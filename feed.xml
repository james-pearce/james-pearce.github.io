<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.0">Jekyll</generator><link href="http://www.3crownsconsulting.com.au/feed.xml" rel="self" type="application/atom+xml" /><link href="http://www.3crownsconsulting.com.au/" rel="alternate" type="text/html" /><updated>2021-06-09T21:56:23+00:00</updated><id>http://www.3crownsconsulting.com.au/feed.xml</id><title type="html">Data Science in the Wild</title><subtitle>Better techniques for getting more from your data.</subtitle><entry><title type="html">Why we do—and don’t—need explainable AI</title><link href="http://www.3crownsconsulting.com.au/machine-learning/xai/shap/2021/06/08/explainable-ai.html" rel="alternate" type="text/html" title="Why we do—and don’t—need explainable AI" /><published>2021-06-08T22:00:00+00:00</published><updated>2021-06-08T22:00:00+00:00</updated><id>http://www.3crownsconsulting.com.au/machine-learning/xai/shap/2021/06/08/explainable-ai</id><content type="html" xml:base="http://www.3crownsconsulting.com.au/machine-learning/xai/shap/2021/06/08/explainable-ai.html">&lt;p&gt;&lt;em&gt;Why explainable AI (known as XAI) is becoming a must-have component of data science and why we may not have come as far as we think.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;A black-box model is no longer good enough for your data scientists, your business or your customers.&lt;/strong&gt;&lt;/p&gt;

&lt;h2 id=&quot;a-data-science-history-perspective&quot;&gt;A data science history perspective&lt;/h2&gt;

&lt;p&gt;Back in the old days (really just a few years ago for some) building models for classification was simple. &lt;a href=&quot;https://en.wikipedia.org/wiki/Generalized_linear_model&quot;&gt;Generalised linear models&lt;/a&gt; (GLMs), and in particular, &lt;a href=&quot;https://en.wikipedia.org/wiki/Logistic_regression&quot;&gt;logistic regression&lt;/a&gt; models were the only algorithms we used.&lt;/p&gt;

&lt;p&gt;And this is the &lt;em&gt;real&lt;/em&gt;, statistical version of logistic regression as you would find in the &lt;a href=&quot;https://www.python.org&quot;&gt;Python&lt;/a&gt; library &lt;a href=&quot;https://www.statsmodels.org/stable/glm.html&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;statsmodels&lt;/code&gt;&lt;/a&gt;, rather than the &lt;a href=&quot;https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;scikit-learn&lt;/code&gt;&lt;/a&gt; version. In the statistical version, you get neat statistical parameter information including &lt;a href=&quot;https://www.sciencedirect.com/topics/engineering/regression-coefficient&quot;&gt;coefficient estimates&lt;/a&gt;, &lt;a href=&quot;https://en.wikipedia.org/wiki/Standard_error&quot;&gt;standard errors&lt;/a&gt; and estimates of fit such as &lt;a href=&quot;https://en.wikipedia.org/wiki/Deviance_(statistics)&quot;&gt;deviance&lt;/a&gt; or &lt;a href=&quot;https://online.stat.psu.edu/stat504/lesson/1/1.5&quot;&gt;log-likelihood&lt;/a&gt; estimates.&lt;/p&gt;

&lt;p&gt;Our models contained a few, essential &lt;a href=&quot;https://en.wikipedia.org/wiki/Feature_%28machine_learning%29&quot;&gt;features&lt;/a&gt;. Those features we did use were handcrafted to be meaningful, stable and predictable.&lt;/p&gt;

&lt;h2 id=&quot;the-new-kid-in-town&quot;&gt;The new kid in town&lt;/h2&gt;

&lt;p&gt;Then things changed. They got more sophisticated and more complex (possibly two words meaning the same thing). We were given a choice of many different algorithms, such as &lt;a href=&quot;https://scikit-learn.org/stable/modules/neural_networks_supervised.html&quot;&gt;neural networks&lt;/a&gt;, &lt;a href=&quot;https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html&quot;&gt;random forests&lt;/a&gt;, &lt;a href=&quot;https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html&quot;&gt;gradient boosting machines&lt;/a&gt;, &lt;a href=&quot;https://scikit-learn.org/stable/modules/svm.html&quot;&gt;support vector machines&lt;/a&gt; and &lt;a href=&quot;https://scikit-learn.org/stable/modules/ensemble.html&quot;&gt;ensemble methods&lt;/a&gt;. We could now use hundreds — or even thousands — of features. In many cases our algorithms would select the best features and even pre-process them, meaning we no longer had to hand-craft them. We started to use a whole bunch of techniques that, when combined, would wring every drop of predictive power from our models and data.&lt;/p&gt;

&lt;p&gt;The mantra was sometimes heard that as long as the model predicted well, why should we care about how the model arrived at its predictions?&lt;/p&gt;

&lt;p&gt;And our debugging and diagnostic techniques evolved accordingly. They did not try to explain &lt;em&gt;why&lt;/em&gt; the model produced a prediction. Rather, &lt;a href=&quot;https://www.datasciencecentral.com/profiles/blogs/7-important-model-evaluation-error-metrics-everyone-should-know&quot;&gt;they quantified the extent to which the algorithm would generalise to a set of unseen data&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;constraints-can-be-good-for-creativity&quot;&gt;Constraints can be good for creativity&lt;/h2&gt;

&lt;p&gt;Admittedly, in the scenario I presented in the old days, a lot of these constraints were thrust upon us. Our software did not support advanced algorithms. The lack of computational power made thousands of features impractical and time-consuming. Putting models into operational systems was hard; it often required hand-coding the algorithm into a mainframe (and yes, this is something I have done &lt;em&gt;in the last decade&lt;/em&gt;). Altogether this pushed the data scientist towards simpler models and implementations that were easier to test.&lt;/p&gt;

&lt;p&gt;So with all these constraints in place, the models and algorithms developed with understanding and interpretability first and foremost. Contributing to why this needed to be so was what the models were used for: things like determining whether to grant an applicant a credit card, for example. It &lt;em&gt;just felt safer&lt;/em&gt; that humans could understand these models, regardless of the checks and balances in the process. We were risking hundreds and thousands of dollars if we got it wrong.&lt;/p&gt;

&lt;p&gt;But for now, enough &lt;em&gt;blah, blah, blah.&lt;/em&gt; I will show you what I mean using the classic &lt;a href=&quot;https://www.kaggle.com/hesh97/titanicdataset-traincsv&quot;&gt;&lt;em&gt;Titanic&lt;/em&gt; data set&lt;/a&gt;.&lt;/p&gt;

&lt;hr /&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Spoiler alert&lt;/strong&gt;&lt;/p&gt;

  &lt;h2 id=&quot;the-ship-hits-an-iceberg-and-sinks&quot;&gt;The ship hits an iceberg and sinks.&lt;/h2&gt;
&lt;/blockquote&gt;

&lt;p&gt;We want to build a model that will predict who would survive on the &lt;em&gt;Titanic&lt;/em&gt;.&lt;sup id=&quot;fnref:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/modelling.svg&quot; alt=&quot;modelling&quot; /&gt;&lt;/p&gt;

&lt;p&gt;To build a model, I use a logistic regression in &lt;a href=&quot;http://cran.r-project.org&quot;&gt;R&lt;/a&gt; using the &lt;a href=&quot;https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/glm&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;glm&lt;/code&gt;&lt;/a&gt; package. You can see my code &lt;a href=&quot;https://github.com/james-pearce/titanic-xai&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Comparing our predictions with what transpired, we get the following misclassification table (or &lt;em&gt;confusion matrix&lt;/em&gt;).&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt; &lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;Actual&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt; &lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;**Predicted**&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;**did not survive**&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;**survived**&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;**did not survive**&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;480&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;84&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;**survived**&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;69&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;258&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;If you add up the numbers, you can see this model gives an accuracy on the data set it was trained on of 82%. This is the number of times we made a prediction that matched with what actually happened.&lt;/p&gt;

&lt;p&gt;Great! Our model is somewhat accurate. But it does not help me in my quest for surviving when &lt;em&gt;Titanic II&lt;/em&gt; hits an iceberg. For that we have to look into the internals of the fitted model and do some simple maths.&lt;/p&gt;

&lt;h3 id=&quot;analysis-of-deviance&quot;&gt;Analysis of deviance&lt;/h3&gt;

&lt;p&gt;First, I look at the &lt;a href=&quot;https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/anova.glm&quot;&gt;analysis of deviance&lt;/a&gt; table. This tells me that all the variables except &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Fare&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Parch&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Embarked&lt;/code&gt; contribute to reducing the variation or error. It also tells me the bulk of predictive power lives in two variables: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Sex&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Pclass&lt;/code&gt;.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt; &lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;Df&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;Deviance&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;Resid. Df&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;Resid. Dev&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;\$\Pr(&amp;gt;\chi^2)$&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;NULL&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;NA&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;NA&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;890&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;1186.7&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;NA&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Pclass&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;2&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;103.5&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;888&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;1083.1&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.00&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Sex&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;256.2&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;887&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;826.9&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.00&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;age_band&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;7&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;25.6&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;880&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;801.2&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.00&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;SibSp&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;2&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;16.7&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;878&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;784.6&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.00&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Parch&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;2&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;3.9&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;876&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;780.7&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.14&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Fare&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;4&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;1.4&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;872&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;779.2&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.84&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Cabin&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;7&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;15.7&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;865&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;763.5&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.03&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Embarked&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;2&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;5.0&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;863&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;758.5&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.08&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h3 id=&quot;table-of-coefficients&quot;&gt;Table of coefficients&lt;/h3&gt;

&lt;p&gt;Fitting a generalised linear model also gives us a table of coefficients. We can use this to see which values lead to greater or lesser chances of survival.&lt;/p&gt;

&lt;div class=&quot;language-R highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;## Call:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;## glm(formula = Survived ~ Pclass + Sex + age_band + SibSp + Parch +&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;##     Fare + Cabin + Embarked, family = binomial, data = titanic_df)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;##&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;## Deviance Residuals:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;##     Min       1Q   Median       3Q      Max&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;## -2.5514  -0.6254  -0.3818   0.5720   2.5841&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;##&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;## Coefficients:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;##                         Estimate Std. Error z value Pr(&amp;gt;|z|)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;## (Intercept)              0.10832    0.77843   0.139 0.889334&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;## Pclass1                  1.39381    0.52904   2.635 0.008424 **&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;## Pclass2                  1.05813    0.30546   3.464 0.000532 ***&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;## Sexfemale                2.73557    0.21185  12.913  &amp;lt; 2e-16 ***&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;## age_band 2. 18 to 24    -1.36944    0.37236  -3.678 0.000235 ***&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;## age_band 3. 25 to 34    -1.08831    0.35717  -3.047 0.002311 **&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;## age_band 4. 35 to 44    -1.53207    0.39933  -3.837 0.000125 ***&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;## age_band 5. 45 to 54    -2.01319    0.45186  -4.455 8.38e-06 ***&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;## age_band 6. 55 to 64    -2.21546    0.60183  -3.681 0.000232 ***&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;## age_band 7. 65 and over -3.35952    1.15066  -2.920 0.003504 **&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;## age_band 8. Other       -1.37755    0.37881  -3.636 0.000276 ***&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;## SibSp1                  -0.02670    0.25043  -0.107 0.915082&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;## SibSp2+                 -1.30675    0.42860  -3.049 0.002297 **&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;## Parch1                   0.17985    0.31417   0.572 0.567005&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;## Parch2+                 -0.42993    0.35948  -1.196 0.231706&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;## Fare 2. ( 10,  20]       0.09089    0.32287   0.282 0.778322&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;## Fare 3. ( 20,  30]       0.06864    0.39740   0.173 0.862865&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;## Fare 4. ( 30,  40]       0.09779    0.49297   0.198 0.842758&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;## Fare 5. ( 40,    ]       0.34598    0.50561   0.684 0.493798&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;## CabinB                   0.06157    0.71413   0.086 0.931292&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;## CabinC                  -0.43524    0.66889  -0.651 0.515244&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;## CabinD                   0.35476    0.75164   0.472 0.636945&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;## CabinE                   0.88119    0.77044   1.144 0.252730&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;## CabinF                   0.39872    1.01748   0.392 0.695157&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;## CabinG/T                -1.94008    1.28924  -1.505 0.132368&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;## CabinNot given          -0.84486    0.68418  -1.235 0.216887&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;## EmbarkedQ                0.04961    0.40881   0.121 0.903410&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;## EmbarkedS               -0.47637    0.25177  -1.892 0.058481 .&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;## ---&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;##&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;## (Dispersion parameter for binomial family taken to be 1)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;##&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;##     Null deviance: 1186.66  on 890  degrees of freedom&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;## Residual deviance:  758.52  on 863  degrees of freedom&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;## AIC: 814.52&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;##&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;## Number of Fisher Scoring iterations: 5&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This tells us:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Pclass&lt;/code&gt; of 3 is associated with non-survival;&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Sex&lt;/code&gt; of ‘female’ is associated with survival;&lt;/li&gt;
  &lt;li&gt;Children (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Age&lt;/code&gt; less than 18) are more likely to survive; and&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Cabin&lt;/code&gt; designations beginning with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;G&lt;/code&gt; are unlikely to survive,&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;as well as some other, less predictive, nuances.&lt;/p&gt;

&lt;h3 id=&quot;explaining-predictions&quot;&gt;Explaining predictions&lt;/h3&gt;

&lt;p&gt;Now we have managed to understand what is happening in our Titanic model. The next step is to &lt;em&gt;understand&lt;/em&gt; why the model makes individual predictions.&lt;/p&gt;

&lt;p&gt;Let’s start by looking at the prediction of the passenger who was predicted to me &lt;em&gt;most&lt;/em&gt; likely to survive. Here are the details of that passenger, who has been predicted as 99.2% likely to survive.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;Pclass&lt;fct&gt;&lt;/fct&gt;&lt;/th&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;Sex&lt;fct&gt;&lt;/fct&gt;&lt;/th&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;age_band&lt;fct&gt;&lt;/fct&gt;&lt;/th&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;SibSp&lt;fct&gt;&lt;/fct&gt;&lt;/th&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;Parch&lt;fct&gt;&lt;/fct&gt;&lt;/th&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;Fare&lt;fct&gt;&lt;/fct&gt;&lt;/th&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;Cabin&lt;fct&gt;&lt;/fct&gt;&lt;/th&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;Embarked&lt;fct&gt;&lt;/fct&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;female&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;1. Under 18&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;0&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;5. ( 40, ]&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;B&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;C&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;All we need to do is a bit of mathematics to understand why this was predicted.&lt;/p&gt;

&lt;div class=&quot;language-R highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;m&quot;&gt;0.10831603&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1.39380902&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;# Pclass 1&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;2.73556786&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;# Sex female&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;# age_band 1. Under 18&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;# SibSp 0&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;0.17985123&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;# Parch 1&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;0.34597961&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;# Fare 40+&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;0.06157172&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;# Cabin B&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;  &lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;# Embarked C&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;So, it is because she is female, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Pclass&lt;/code&gt; 1, under 18 and paid fare of over 40, along with some other details.&lt;/p&gt;

&lt;p&gt;Now let’s look at the passenger predicted to be least likely to be a survivor. Here are the details of that passenger, predicted with a 1.0% chance of surviving.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;Pclass&lt;fct&gt;&lt;/fct&gt;&lt;/th&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;Sex&lt;fct&gt;&lt;/fct&gt;&lt;/th&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;age_band&lt;fct&gt;&lt;/fct&gt;&lt;/th&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;SibSp&lt;fct&gt;&lt;/fct&gt;&lt;/th&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;Parch&lt;fct&gt;&lt;/fct&gt;&lt;/th&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;Fare&lt;fct&gt;&lt;/fct&gt;&lt;/th&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;Cabin&lt;fct&gt;&lt;/fct&gt;&lt;/th&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;Embarked&lt;fct&gt;&lt;/fct&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;3&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;male&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;7. 65 and over&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;0&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;0&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;1. ( 0, 10]&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Not given&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;S&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Again, we can see why this prediction was made with a bit of maths.&lt;/p&gt;

&lt;div class=&quot;language-R highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;m&quot;&gt;0.10831603&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;# Pclass 3&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;# Sex male&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;-3.35951828&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;# age_band 7. 65 and over&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;# SibSp 0&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;# Parch 0&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;# 1. (0, 10]&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;-0.84485644&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;# Cabin Not given&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;-0.47636764&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;  &lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;# Embarked S&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;It is because&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;the passenger was male;&lt;/li&gt;
  &lt;li&gt;he was aged 65 or over;&lt;/li&gt;
  &lt;li&gt;he was in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Pclass&lt;/code&gt; 3; and&lt;/li&gt;
  &lt;li&gt;paid a low fare,&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;as well as other attributes that contribute to the low prediction.&lt;/p&gt;

&lt;h3 id=&quot;interpretable-and-explainable&quot;&gt;Interpretable and explainable&lt;/h3&gt;

&lt;p&gt;It is interesting to note that the old-school approaches grounded in statistics gave us models that were &lt;em&gt;interpretable&lt;/em&gt; and produced predictions that were &lt;em&gt;explainable&lt;/em&gt;.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;modern-machine-learning&quot;&gt;Modern machine learning&lt;/h2&gt;

&lt;p&gt;Move forward to today. Machine learning is moving to the mainstream and there are a plethora of tools we can use to build a predictive mode. We are no longer confined to ‘just’ the regression family (although there is a new, improved machine-learning version of regression, too).&lt;/p&gt;

&lt;p&gt;The data scientist can choose from a dazzling array of algorithms. As an example, &lt;a href=&quot;https://lazypredict.readthedocs.io/en/latest/&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;lazypredict&lt;/code&gt;&lt;/a&gt; will automatically fit &lt;a href=&quot;https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;LinearSVC&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;SGDClassifier&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html?highlight=logisticregression#sklearn.linear_model.LogisticRegression&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;LogisticRegression&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html?highlight=randomforestclassifier#sklearn.ensemble.RandomForestClassifier&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;RandomForestClassifier&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html?highlight=gradientboostingclassifier#sklearn.ensemble.GradientBoostingClassifier&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;GradientBoostingClassifier&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html?highlight=gaussiannb#sklearn.naive_bayes.GaussianNB&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;GaussianNB&lt;/code&gt;&lt;/a&gt; and another &lt;em&gt;twenty-five more!&lt;/em&gt;&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;o&quot;&gt;|&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Model&lt;/span&gt;                          &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;   &lt;span class=&quot;n&quot;&gt;Accuracy&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;   &lt;span class=&quot;n&quot;&gt;Balanced&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Accuracy&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;   &lt;span class=&quot;n&quot;&gt;ROC&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;AUC&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;   &lt;span class=&quot;n&quot;&gt;F1&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Score&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;   &lt;span class=&quot;n&quot;&gt;Time&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Taken&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-------------------------------|-----------&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;|--------------------&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;|----------&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;|-----------&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;|-------------&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;|&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;LinearSVC&lt;/span&gt;                      &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;   &lt;span class=&quot;mf&quot;&gt;0.989474&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;            &lt;span class=&quot;mf&quot;&gt;0.987544&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;0.987544&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;   &lt;span class=&quot;mf&quot;&gt;0.989462&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;    &lt;span class=&quot;mf&quot;&gt;0.0150008&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;|&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;SGDClassifier&lt;/span&gt;                  &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;   &lt;span class=&quot;mf&quot;&gt;0.989474&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;            &lt;span class=&quot;mf&quot;&gt;0.987544&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;0.987544&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;   &lt;span class=&quot;mf&quot;&gt;0.989462&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;    &lt;span class=&quot;mf&quot;&gt;0.0109992&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;|&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;MLPClassifier&lt;/span&gt;                  &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;   &lt;span class=&quot;mf&quot;&gt;0.985965&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;            &lt;span class=&quot;mf&quot;&gt;0.986904&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;0.986904&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;   &lt;span class=&quot;mf&quot;&gt;0.985994&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;    &lt;span class=&quot;mf&quot;&gt;0.426&lt;/span&gt;     &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;|&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Perceptron&lt;/span&gt;                     &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;   &lt;span class=&quot;mf&quot;&gt;0.985965&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;            &lt;span class=&quot;mf&quot;&gt;0.984797&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;0.984797&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;   &lt;span class=&quot;mf&quot;&gt;0.985965&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;    &lt;span class=&quot;mf&quot;&gt;0.0120046&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;|&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;LogisticRegression&lt;/span&gt;             &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;   &lt;span class=&quot;mf&quot;&gt;0.985965&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;            &lt;span class=&quot;mf&quot;&gt;0.98269&lt;/span&gt;  &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;0.98269&lt;/span&gt;  &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;   &lt;span class=&quot;mf&quot;&gt;0.985934&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;    &lt;span class=&quot;mf&quot;&gt;0.0200036&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;|&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;LogisticRegressionCV&lt;/span&gt;           &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;   &lt;span class=&quot;mf&quot;&gt;0.985965&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;            &lt;span class=&quot;mf&quot;&gt;0.98269&lt;/span&gt;  &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;0.98269&lt;/span&gt;  &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;   &lt;span class=&quot;mf&quot;&gt;0.985934&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;    &lt;span class=&quot;mf&quot;&gt;0.262997&lt;/span&gt;  &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;|&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;SVC&lt;/span&gt;                            &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;   &lt;span class=&quot;mf&quot;&gt;0.982456&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;            &lt;span class=&quot;mf&quot;&gt;0.979942&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;0.979942&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;   &lt;span class=&quot;mf&quot;&gt;0.982437&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;    &lt;span class=&quot;mf&quot;&gt;0.0140011&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;|&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;CalibratedClassifierCV&lt;/span&gt;         &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;   &lt;span class=&quot;mf&quot;&gt;0.982456&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;            &lt;span class=&quot;mf&quot;&gt;0.975728&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;0.975728&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;   &lt;span class=&quot;mf&quot;&gt;0.982357&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;    &lt;span class=&quot;mf&quot;&gt;0.0350015&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;|&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;PassiveAggressiveClassifier&lt;/span&gt;    &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;   &lt;span class=&quot;mf&quot;&gt;0.975439&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;            &lt;span class=&quot;mf&quot;&gt;0.974448&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;0.974448&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;   &lt;span class=&quot;mf&quot;&gt;0.975464&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;    &lt;span class=&quot;mf&quot;&gt;0.0130005&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;|&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;LabelPropagation&lt;/span&gt;               &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;   &lt;span class=&quot;mf&quot;&gt;0.975439&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;            &lt;span class=&quot;mf&quot;&gt;0.974448&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;0.974448&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;   &lt;span class=&quot;mf&quot;&gt;0.975464&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;    &lt;span class=&quot;mf&quot;&gt;0.0429988&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;|&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;LabelSpreading&lt;/span&gt;                 &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;   &lt;span class=&quot;mf&quot;&gt;0.975439&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;            &lt;span class=&quot;mf&quot;&gt;0.974448&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;0.974448&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;   &lt;span class=&quot;mf&quot;&gt;0.975464&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;    &lt;span class=&quot;mf&quot;&gt;0.0310006&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;|&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;RandomForestClassifier&lt;/span&gt;         &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;   &lt;span class=&quot;mf&quot;&gt;0.97193&lt;/span&gt;  &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;            &lt;span class=&quot;mf&quot;&gt;0.969594&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;0.969594&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;   &lt;span class=&quot;mf&quot;&gt;0.97193&lt;/span&gt;  &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;    &lt;span class=&quot;mf&quot;&gt;0.033&lt;/span&gt;     &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;|&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;GradientBoostingClassifier&lt;/span&gt;     &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;   &lt;span class=&quot;mf&quot;&gt;0.97193&lt;/span&gt;  &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;            &lt;span class=&quot;mf&quot;&gt;0.967486&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;0.967486&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;   &lt;span class=&quot;mf&quot;&gt;0.971869&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;    &lt;span class=&quot;mf&quot;&gt;0.166998&lt;/span&gt;  &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;|&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;QuadraticDiscriminantAnalysis&lt;/span&gt;  &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;   &lt;span class=&quot;mf&quot;&gt;0.964912&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;            &lt;span class=&quot;mf&quot;&gt;0.966206&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;0.966206&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;   &lt;span class=&quot;mf&quot;&gt;0.965052&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;    &lt;span class=&quot;mf&quot;&gt;0.0119994&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;|&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;HistGradientBoostingClassifier&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;   &lt;span class=&quot;mf&quot;&gt;0.968421&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;            &lt;span class=&quot;mf&quot;&gt;0.964739&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;0.964739&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;   &lt;span class=&quot;mf&quot;&gt;0.968387&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;    &lt;span class=&quot;mf&quot;&gt;0.682003&lt;/span&gt;  &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;|&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;RidgeClassifierCV&lt;/span&gt;              &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;   &lt;span class=&quot;mf&quot;&gt;0.97193&lt;/span&gt;  &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;            &lt;span class=&quot;mf&quot;&gt;0.963272&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;0.963272&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;   &lt;span class=&quot;mf&quot;&gt;0.971736&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;    &lt;span class=&quot;mf&quot;&gt;0.0130029&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;|&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;RidgeClassifier&lt;/span&gt;                &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;   &lt;span class=&quot;mf&quot;&gt;0.968421&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;            &lt;span class=&quot;mf&quot;&gt;0.960525&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;0.960525&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;   &lt;span class=&quot;mf&quot;&gt;0.968242&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;    &lt;span class=&quot;mf&quot;&gt;0.0119977&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;|&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;AdaBoostClassifier&lt;/span&gt;             &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;   &lt;span class=&quot;mf&quot;&gt;0.961404&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;            &lt;span class=&quot;mf&quot;&gt;0.959245&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;0.959245&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;   &lt;span class=&quot;mf&quot;&gt;0.961444&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;    &lt;span class=&quot;mf&quot;&gt;0.204998&lt;/span&gt;  &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;|&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ExtraTreesClassifier&lt;/span&gt;           &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;   &lt;span class=&quot;mf&quot;&gt;0.961404&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;            &lt;span class=&quot;mf&quot;&gt;0.957138&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;0.957138&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;   &lt;span class=&quot;mf&quot;&gt;0.961362&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;    &lt;span class=&quot;mf&quot;&gt;0.0270066&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;|&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;KNeighborsClassifier&lt;/span&gt;           &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;   &lt;span class=&quot;mf&quot;&gt;0.961404&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;            &lt;span class=&quot;mf&quot;&gt;0.95503&lt;/span&gt;  &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;0.95503&lt;/span&gt;  &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;   &lt;span class=&quot;mf&quot;&gt;0.961276&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;    &lt;span class=&quot;mf&quot;&gt;0.0560005&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;|&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;BaggingClassifier&lt;/span&gt;              &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;   &lt;span class=&quot;mf&quot;&gt;0.947368&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;            &lt;span class=&quot;mf&quot;&gt;0.954577&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;0.954577&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;   &lt;span class=&quot;mf&quot;&gt;0.947882&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;    &lt;span class=&quot;mf&quot;&gt;0.0559971&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;|&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;BernoulliNB&lt;/span&gt;                    &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;   &lt;span class=&quot;mf&quot;&gt;0.950877&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;            &lt;span class=&quot;mf&quot;&gt;0.951003&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;0.951003&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;   &lt;span class=&quot;mf&quot;&gt;0.951072&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;    &lt;span class=&quot;mf&quot;&gt;0.0169988&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;|&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;LinearDiscriminantAnalysis&lt;/span&gt;     &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;   &lt;span class=&quot;mf&quot;&gt;0.961404&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;            &lt;span class=&quot;mf&quot;&gt;0.950816&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;0.950816&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;   &lt;span class=&quot;mf&quot;&gt;0.961089&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;    &lt;span class=&quot;mf&quot;&gt;0.0199995&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;|&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;GaussianNB&lt;/span&gt;                     &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;   &lt;span class=&quot;mf&quot;&gt;0.954386&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;            &lt;span class=&quot;mf&quot;&gt;0.949536&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;0.949536&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;   &lt;span class=&quot;mf&quot;&gt;0.954337&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;    &lt;span class=&quot;mf&quot;&gt;0.0139935&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;|&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;NuSVC&lt;/span&gt;                          &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;   &lt;span class=&quot;mf&quot;&gt;0.954386&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;            &lt;span class=&quot;mf&quot;&gt;0.943215&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;0.943215&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;   &lt;span class=&quot;mf&quot;&gt;0.954014&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;    &lt;span class=&quot;mf&quot;&gt;0.019989&lt;/span&gt;  &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;|&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;DecisionTreeClassifier&lt;/span&gt;         &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;   &lt;span class=&quot;mf&quot;&gt;0.936842&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;            &lt;span class=&quot;mf&quot;&gt;0.933693&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;0.933693&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;   &lt;span class=&quot;mf&quot;&gt;0.936971&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;    &lt;span class=&quot;mf&quot;&gt;0.0170023&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;|&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;NearestCentroid&lt;/span&gt;                &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;   &lt;span class=&quot;mf&quot;&gt;0.947368&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;            &lt;span class=&quot;mf&quot;&gt;0.933506&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;0.933506&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;   &lt;span class=&quot;mf&quot;&gt;0.946801&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;    &lt;span class=&quot;mf&quot;&gt;0.0160074&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;|&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ExtraTreeClassifier&lt;/span&gt;            &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;   &lt;span class=&quot;mf&quot;&gt;0.922807&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;            &lt;span class=&quot;mf&quot;&gt;0.912168&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;0.912168&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;   &lt;span class=&quot;mf&quot;&gt;0.922462&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;    &lt;span class=&quot;mf&quot;&gt;0.0109999&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;|&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;CheckingClassifier&lt;/span&gt;             &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;   &lt;span class=&quot;mf&quot;&gt;0.361404&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;            &lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;      &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;      &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;   &lt;span class=&quot;mf&quot;&gt;0.191879&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;    &lt;span class=&quot;mf&quot;&gt;0.0170043&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;|&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;DummyClassifier&lt;/span&gt;                &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;   &lt;span class=&quot;mf&quot;&gt;0.512281&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;            &lt;span class=&quot;mf&quot;&gt;0.489598&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;0.489598&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;   &lt;span class=&quot;mf&quot;&gt;0.518924&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;    &lt;span class=&quot;mf&quot;&gt;0.0119965&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;A list of classifiers from &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;lazypredict&lt;/code&gt;’s documentation.&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;What a long way we have come …&lt;/p&gt;

&lt;h3 id=&quot;-or-have-we&quot;&gt;… Or have we?&lt;/h3&gt;

&lt;p&gt;My original analytical training was as a statistician. This means I have a bias, so please keep that in mind.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://external-content.duckduckgo.com/iu/?u=https%3A%2F%2Ftse4.mm.bing.net%2Fth%3Fid%3DOIP.SDJJ0y7zXX5HVwuZXBIaQwHaDf%26pid%3DApi&amp;amp;f=1&quot; alt=&quot;A statistician quotation&quot; /&gt;&lt;/p&gt;

&lt;p&gt;With the classical, statistical techniques the focus was on &lt;em&gt;inference&lt;/em&gt;; the new machine learning techniques focus on &lt;em&gt;prediction&lt;/em&gt;. For a while there was a sentiment—and indeed, a junior aspiring data scientist said this to me—that so long as a model predicts well, it does not matter what happens inside the algorithm’s black box.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;The end justifies the means.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Of course, now we know that it &lt;em&gt;does&lt;/em&gt; matter what happens; we do need to be able to understand &lt;em&gt;what&lt;/em&gt; we are doing.&lt;/p&gt;

&lt;p&gt;First, we need to be sure that an algorithm predicts well. If we do not understand the algorithm, how can we know this?&lt;/p&gt;

&lt;p&gt;We can look at the predictions made against our validation data set. (I am sure some business stakeholders relying on models performing well would be horrified to learn they might have been tested only against a couple of hundred records to make sure they behaved as expected.) But this will not tell us if we have made &lt;a href=&quot;https://www.reuters.com/article/us-amazon-com-jobs-automation-insight/amazon-scraps-secret-ai-recruiting-tool-that-showed-bias-against-women-idUSKCN1MK08G&quot;&gt;a catastrophic error in the selection of our data&lt;/a&gt;. It will not tell us if there are subsegments of our customer base for which the predictions do not work well. When the algorithm is a ‘black box’ which simply takes inputs and returns outputs with no clue as to its inner workings, the only debugging possible is to run many cases through and make sure the prediction is close to the truth.&lt;/p&gt;

&lt;p&gt;Second, we need to be able to explain &lt;em&gt;why&lt;/em&gt; we made individual predictions. I am sure you would agree that it is not satistying to explain a refusal to accept a loan application on the grounds of ‘computer says no’. Neither the customer nor the customer service agent would have a reason to trust the algorithm. They are left to guessing why the application was not accepted.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://external-content.duckduckgo.com/iu/?u=https%3A%2F%2Ftse3.mm.bing.net%2Fth%3Fid%3DOIP.LHL6AyEugKKtdom-rCQTcQHaEV%26pid%3DApi&amp;amp;f=1&quot; alt=&quot;Computer says no&quot; /&gt;&lt;/p&gt;

&lt;p&gt;But that is what we are asking people to do when we say ’trust the machine’. We are asking humans in a real process making operational decisions to use the predictions from a machine learning algorithm. Surely it would be in the interest of everyone invested in the model’s success to ensure the users trust and understand why the predictions are model. They need to get reasons of why this customer has been selected to receive this discount, or why a particular machine has been selected for maintenance. If they cannot trust it, adoption rates and compliance rates will remain low.&lt;/p&gt;

&lt;p&gt;Third, there is an increasing focus on models being well governed and responsible. Initiatives like &lt;a href=&quot;https://www.oreilly.com/radar/how-will-the-gdpr-impact-machine-learning/&quot;&gt;GDPR&lt;/a&gt; prescribe how machine learning is governed, how users’ consent is managed and how interpretable a model’s predictions are. In parallel, there is increasing focus on &lt;a href=&quot;https://consult.industry.gov.au/strategic-policy/artificial-intelligence-ethics-framework/supporting_documents/ArtificialIntelligenceethicsframeworkdiscussionpaper.pdf&quot;&gt;ethical AI&lt;/a&gt; and &lt;a href=&quot;https://www.microsoft.com/en-us/ai/responsible-ai?activetab=pivot1:primaryr6&quot;&gt;responsible AI&lt;/a&gt;, which include a set of principles to ensure machine learning models used by organisations are&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;fair;&lt;/li&gt;
  &lt;li&gt;reliable and safe;&lt;/li&gt;
  &lt;li&gt;inclusive;&lt;/li&gt;
  &lt;li&gt;transparent;&lt;/li&gt;
  &lt;li&gt;private and secure; and&lt;/li&gt;
  &lt;li&gt;have clear accountability.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;what-have-we-lost&quot;&gt;What have we lost?&lt;/h3&gt;

&lt;p&gt;With the classical statistical techniques of yore, we had measure of uncretainty around the parameters of the algorithm. We could tell where it was accurate and where it was less so. We could calculate the predictions with a modest amount of matrix mathematics.&lt;/p&gt;

&lt;p&gt;The new techniques kind of forgot about these things. Or they added them as an afterthought.&lt;/p&gt;

&lt;p&gt;Now, though, there is something of a renaissance brewing under the name of XAI—explainable AI. XAI typically refers to a suite of tools and technique that let us &lt;em&gt;interpret&lt;/em&gt; fitted models and &lt;em&gt;explain&lt;/em&gt; predictions of almost any model.&lt;/p&gt;

&lt;h3 id=&quot;shap&quot;&gt;SHAP&lt;/h3&gt;

&lt;p&gt;The new hero of the day is a set of libraries called &lt;a href=&quot;https://shap.readthedocs.io/en/latest/&quot;&gt;SHAP&lt;/a&gt;—&lt;strong&gt;SH&lt;/strong&gt;apley &lt;strong&gt;a&lt;/strong&gt;dditive &lt;strong&gt;p&lt;/strong&gt;redictions.&lt;sup id=&quot;fnref:2&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;p&gt;What SHAP does, in essence, is to run a series of test observations through a model’s prediction algorithm to see what happens. The reason its use is increasing so rapidly is that it outputs a series of additive outputs. (You know, like the regression models do.)&lt;/p&gt;

&lt;p&gt;And additive outputs are easy to interpret. You can add them up, take the mean (a statistician’s way of saying ‘average’); they behave sensibly and intuitively.&lt;/p&gt;

&lt;h3 id=&quot;back-on-board-the-titanic&quot;&gt;Back on board the &lt;em&gt;Titanic&lt;/em&gt;&lt;/h3&gt;

&lt;p&gt;So to show you how we can apply SHAP to the same data set as before, I used the Python &lt;a href=&quot;https://xgboost.readthedocs.io/en/latest/python/python_api.html#module-xgboost.sklearn&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;XGBClassifier&lt;/code&gt;&lt;/a&gt; against the data. I used the default parameters and the same feature transformations I used with the logistic regression model.&lt;/p&gt;

&lt;p&gt;Instead of an analysis of deviance table, all the model fitting process gives me is a series of sequential trees that reduce the variation of error at each iteration. &lt;a href=&quot;https://en.wikipedia.org/wiki/Gradient_boosting&quot;&gt;This is what GBMs do.&lt;/a&gt; I can find out which features contribute to the model using one of the ‘afterthought’ techniques included with the algorithm.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/feature-importance.png&quot; alt=&quot;feature-importance&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;The most important features as shown by the GBM afterthought.&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Note that in what is produced above, we get no sense of the bounds of error or variation.&lt;/p&gt;

&lt;p&gt;To get some more information, we can turn to SHAP. It turns out that SHAP has a similar plot that also shows the individual points in the set of test observations we have used.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/shap-summary-plot.png&quot; alt=&quot;shap-summary-plot&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This is a bit more informative. We can see perfect separation on the basis of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Sex&lt;/code&gt;; that there are some bad values of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Pclass&lt;/code&gt;; that being old was not a good thing.&lt;/p&gt;

&lt;h3 id=&quot;explaining-predictions-1&quot;&gt;Explaining predictions&lt;/h3&gt;

&lt;p&gt;As we did before, we can look at explaining individual predictions. SHAP can explain the prediction for the passenger most likely to survive.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/best-passenger.png&quot; alt=&quot;best-passenger&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Now that chart is easy to understand (for a data scientist).&lt;/p&gt;

&lt;p&gt;Similarly for the unluckiest passenger.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/worst-passenger.png&quot; alt=&quot;worst-passenger&quot; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: with a bit more mathematics, we could have represented the outputs of the logistic regression in exactly the same way.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;comparing-the-old-and-the-new&quot;&gt;Comparing the old and the new&lt;/h2&gt;

&lt;p&gt;So now you have seen what we used to do and what we can do now with SHAP.&lt;/p&gt;

&lt;p&gt;“So,” I hear you saying, “you are building a second, additive and interpretable model to explain a model that used a technique that is inherently difficult to explain.”&lt;/p&gt;

&lt;p&gt;Yes, that is right—now we have two models. One to predict, and one to explain the predictions and interpret the model.&lt;/p&gt;

&lt;p&gt;“Wow,” you must be thinking, “these new-fangled machine learning techniques must be &lt;em&gt;really something&lt;/em&gt; for you to add the complexity of having two models instead of one.”&lt;/p&gt;

&lt;h2 id=&quot;back-to-you-kaggle&quot;&gt;Back to you, Kaggle&lt;/h2&gt;

&lt;p&gt;To show you how far we have come, I tested these models on Kaggle (and there are many, many models on Kaggle that predict better on this data set). Kaggle provides a test set of data that contains all the variables of the training data set, but does not include the information on whether an individual survived or not. Kaggle uses this unseen information to calculate a performance score. It uses a measure of accuracy, and higher means better.&lt;/p&gt;

&lt;p&gt;First I ran the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;XGBClassifier&lt;/code&gt; model’s predictions through Kaggle. The result: 0.734 accuracy.&lt;/p&gt;

&lt;p&gt;Next I tried the logistic regression. The result: 0.754 accuracy.&lt;/p&gt;

&lt;p&gt;Looks like we need to do more work to explain our new, fancy machine learning techniques but get little gain in this instance.&lt;/p&gt;

&lt;p&gt;The takeaway might be to think about your model development lifecycle:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;build an interpretable model using classical, interpretable techniques;&lt;/li&gt;
  &lt;li&gt;Once you are happy with this, use a modern machine learning technique to see if you get a significant gain in performance; and&lt;/li&gt;
  &lt;li&gt;If you do, consider whether you want to spend more time tweaking the interpretable model or explaining the machine learning model. Most times you won’t.&lt;/li&gt;
&lt;/ol&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;Increasingly data scientists and the systems that use the models produced by data scientists are under pressure to be well-governed, interpretable and well-understood. Their predictions need to be, well, predictable.&lt;/p&gt;

&lt;p&gt;The SHAP library is very useful and gives us insight into models that would otherwise have been obscured from our view. But for some use cases, you are better off using the classical techniques and understanding your model as you develop it. The alternative road can be a lot of complexity and effort for less understanding and no gain in prediction accuracy.&lt;/p&gt;
&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;A prediction that will be really helpful if you want to book safe passage on &lt;em&gt;Titanic II&lt;/em&gt;’s maiden voyage. &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:2&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;An in-depth treatment of this can be found in &lt;a href=&quot;https://christophm.github.io/interpretable-ml-book/&quot;&gt;Christoph Molnar’s e-Book &lt;em&gt;Interpretable Machine Learning&lt;/em&gt;&lt;/a&gt;. &lt;a href=&quot;#fnref:2&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;</content><author><name></name></author><category term="machine-learning" /><category term="XAI" /><category term="SHAP" /><summary type="html">Why explainable AI (known as XAI) is becoming a must-have component of data science and why we may not have come as far as we think.</summary></entry><entry><title type="html">New features in scikit-learn part 1 — successive halving</title><link href="http://www.3crownsconsulting.com.au/machine-learning/sklearn/2021/02/15/scikit-learn-successive-halving-example.html" rel="alternate" type="text/html" title="New features in scikit-learn part 1 — successive halving" /><published>2021-02-15T22:00:00+00:00</published><updated>2021-02-15T22:00:00+00:00</updated><id>http://www.3crownsconsulting.com.au/machine-learning/sklearn/2021/02/15/scikit-learn-successive-halving-example</id><content type="html" xml:base="http://www.3crownsconsulting.com.au/machine-learning/sklearn/2021/02/15/scikit-learn-successive-halving-example.html">&lt;h1 id=&quot;new-features-in-scikit-learn-024&quot;&gt;New features in scikit-learn 0.24&lt;/h1&gt;

&lt;p&gt;Welcome to a short series of posts where I look at some of the new features introduced in &lt;a href=&quot;https://scikit-learn.org/stable/&quot;&gt;scikit-learn&lt;/a&gt; version 0.24, a newish release of the popular Python machine learning library.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;You can find a copy of this Jupyter notebook &lt;a href=&quot;https://drive.google.com/file/d/1T_7sTm0GvMLJFMTQbxtRJM4HTZWqD0-z/view?usp=sharing&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;feature-1-successive-halving&quot;&gt;Feature #1: Successive Halving&lt;/h2&gt;

&lt;p&gt;The first new feature we will examine is called ‘successive halving’. It is suggested as an alternative to &lt;a href=&quot;https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html&quot;&gt;grid searches&lt;/a&gt; or &lt;a href=&quot;https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html&quot;&gt;random searches&lt;/a&gt; for &lt;a href=&quot;https://en.wikipedia.org/wiki/Hyperparameter_optimization&quot;&gt;hyperparameter tuning&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Note that I prefer random searches in general because&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;the data scientist has more control over the execution time of the search; and&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf&quot;&gt;random searches can lean to a more accurate result&lt;/a&gt; compared with a grid search because they go “in between”
the grid ‘lines’.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The idea with successive halving (which you can read more about &lt;a href=&quot;https://blog.ml.cmu.edu/2018/12/12/massively-parallel-hyperparameter-optimization/&quot;&gt;here&lt;/a&gt;  is that early in the search we use fewer ‘resources’ but search more candidates. ‘Resources’ usually refers to the number of samples used to git a learner, but &lt;em&gt;can&lt;/em&gt; be any parameter. Another sensible choice for multiple-tree-based learners (like &lt;a href=&quot;https://en.wikipedia.org/wiki/Random_forest&quot;&gt;random forests&lt;/a&gt; or &lt;a href=&quot;https://en.wikipedia.org/wiki/Gradient_boosting&quot;&gt;GBMs&lt;/a&gt;) is the number of trees fitted.&lt;/p&gt;

&lt;h1 id=&quot;successive-halving-example&quot;&gt;Successive halving example&lt;/h1&gt;

&lt;p&gt;So now we know what the successive halving does, let’s try it out on some real-world data. Unfortunately the examples in scikit-learn’s examples use only generated data, which I find too clinical to give an indication of what a classification algorithm might be like to use in anger.&lt;/p&gt;

&lt;p&gt;The parameter &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;factor&lt;/code&gt; sets the ‘halving’ or fraction rate. It determines the proportion of candidates that are selected for each subsequent iteration. For example, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;factor=3&lt;/code&gt; means that only one third of the candidates are selected.&lt;/p&gt;

&lt;p&gt;If we do &lt;em&gt;not&lt;/em&gt; specify a specific parameter, the default is to change the size of the sample at each halving iteration. In this way you can select a small sample across the grid space of hyperparameters that gets successively larger as you get closer to the optimum (at least theoretically).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/successive-halving-example.png&quot; alt=&quot;Illustration of successive halving&quot; /&gt;&lt;br /&gt;
&lt;em&gt;&lt;strong&gt;Example of successive halving with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;factor = 2&lt;/code&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;import-the-libraries-we-will-need&quot;&gt;Import the libraries we will need&lt;/h2&gt;

&lt;p&gt;As usual, I like to import a few standard libraries so I can manipulate data frames and access the underlying OS.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;os&lt;/span&gt;

&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;pandas&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;I also like to change the configuration of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pandas&lt;/code&gt; to show all the columns of a data frame.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;set_option&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'display.max_columns'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;lending-club&quot;&gt;Lending club&lt;/h2&gt;

&lt;p&gt;We will use a sample of the &lt;a href=&quot;https://www.lendingclub.com&quot;&gt;&lt;strong&gt;Lending Club&lt;/strong&gt;&lt;/a&gt; data. This is a view of the data I have manipulated a little bit for education purposes, and you can download it &lt;a href=&quot;https://drive.google.com/file/d/1Rahuvn8LwKPNnch5lrhKvEyQHkZGZpuu/view?usp=sharing&quot;&gt;here&lt;/a&gt;. This data set contains details of loans, and we are interested in classifying loans that have gone ‘bad’ (that is, not repaid) against others.&lt;/p&gt;

&lt;p&gt;The data is stored in CSV format, so we read it in using &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pandas&lt;/code&gt;.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;data_path&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'~/repos/sklearn00/data'&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;lending_filename&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'loan_stats_course.csv'&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;lending_df&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;read_csv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;os&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;join&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data_path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lending_filename&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Examine the data.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;lending_df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;head&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div&gt;
&lt;style scoped=&quot;&quot;&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
&lt;/style&gt;
&lt;table border=&quot;1&quot; class=&quot;dataframe&quot;&gt;
  &lt;thead&gt;
    &lt;tr style=&quot;text-align: right;&quot;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;id&lt;/th&gt;
      &lt;th&gt;member_id&lt;/th&gt;
      &lt;th&gt;loan_amnt&lt;/th&gt;
      &lt;th&gt;funded_amnt&lt;/th&gt;
      &lt;th&gt;funded_amnt_inv&lt;/th&gt;
      &lt;th&gt;term&lt;/th&gt;
      &lt;th&gt;int_rate&lt;/th&gt;
      &lt;th&gt;installment&lt;/th&gt;
      &lt;th&gt;grade&lt;/th&gt;
      &lt;th&gt;sub_grade&lt;/th&gt;
      &lt;th&gt;emp_title&lt;/th&gt;
      &lt;th&gt;emp_length&lt;/th&gt;
      &lt;th&gt;home_ownership&lt;/th&gt;
      &lt;th&gt;annual_inc&lt;/th&gt;
      &lt;th&gt;is_inc_v&lt;/th&gt;
      &lt;th&gt;issue_d&lt;/th&gt;
      &lt;th&gt;loan_status&lt;/th&gt;
      &lt;th&gt;pymnt_plan&lt;/th&gt;
      &lt;th&gt;purpose&lt;/th&gt;
      &lt;th&gt;zip_code&lt;/th&gt;
      &lt;th&gt;addr_state&lt;/th&gt;
      &lt;th&gt;dti&lt;/th&gt;
      &lt;th&gt;delinq_2yrs&lt;/th&gt;
      &lt;th&gt;inq_last_6mths&lt;/th&gt;
      &lt;th&gt;mths_since_last_delinq&lt;/th&gt;
      &lt;th&gt;mths_since_last_record&lt;/th&gt;
      &lt;th&gt;open_acc&lt;/th&gt;
      &lt;th&gt;pub_rec&lt;/th&gt;
      &lt;th&gt;revol_bal&lt;/th&gt;
      &lt;th&gt;revol_util&lt;/th&gt;
      &lt;th&gt;total_acc&lt;/th&gt;
      &lt;th&gt;initial_list_status&lt;/th&gt;
      &lt;th&gt;out_prncp&lt;/th&gt;
      &lt;th&gt;out_prncp_inv&lt;/th&gt;
      &lt;th&gt;total_pymnt&lt;/th&gt;
      &lt;th&gt;total_pymnt_inv&lt;/th&gt;
      &lt;th&gt;total_rec_prncp&lt;/th&gt;
      &lt;th&gt;total_rec_int&lt;/th&gt;
      &lt;th&gt;total_rec_late_fee&lt;/th&gt;
      &lt;th&gt;recoveries&lt;/th&gt;
      &lt;th&gt;collection_recovery_fee&lt;/th&gt;
      &lt;th&gt;last_pymnt_amnt&lt;/th&gt;
      &lt;th&gt;next_pymnt_d&lt;/th&gt;
      &lt;th&gt;collections_12_mths_ex_med&lt;/th&gt;
      &lt;th&gt;mths_since_last_major_derog&lt;/th&gt;
      &lt;th&gt;policy_code&lt;/th&gt;
      &lt;th&gt;bad_loan&lt;/th&gt;
      &lt;th&gt;credit_length_in_years&lt;/th&gt;
      &lt;th&gt;earned&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;1077430&lt;/td&gt;
      &lt;td&gt;1314167&lt;/td&gt;
      &lt;td&gt;2500&lt;/td&gt;
      &lt;td&gt;2500&lt;/td&gt;
      &lt;td&gt;2500.0&lt;/td&gt;
      &lt;td&gt;60 months&lt;/td&gt;
      &lt;td&gt;15.27&lt;/td&gt;
      &lt;td&gt;59.83&lt;/td&gt;
      &lt;td&gt;C&lt;/td&gt;
      &lt;td&gt;C4&lt;/td&gt;
      &lt;td&gt;Ryder&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;RENT&lt;/td&gt;
      &lt;td&gt;30000.0&lt;/td&gt;
      &lt;td&gt;Source Verified&lt;/td&gt;
      &lt;td&gt;Dec-2011&lt;/td&gt;
      &lt;td&gt;Charged Off&lt;/td&gt;
      &lt;td&gt;n&lt;/td&gt;
      &lt;td&gt;car&lt;/td&gt;
      &lt;td&gt;309xx&lt;/td&gt;
      &lt;td&gt;GA&lt;/td&gt;
      &lt;td&gt;1.00&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;5.0&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;3.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;1687&lt;/td&gt;
      &lt;td&gt;9.4&lt;/td&gt;
      &lt;td&gt;4.0&lt;/td&gt;
      &lt;td&gt;f&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;1008.710000&lt;/td&gt;
      &lt;td&gt;1008.71&lt;/td&gt;
      &lt;td&gt;456.46&lt;/td&gt;
      &lt;td&gt;435.17&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;117.08&lt;/td&gt;
      &lt;td&gt;1.11&lt;/td&gt;
      &lt;td&gt;119.66&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;12.0&lt;/td&gt;
      &lt;td&gt;-1491.290000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;1077175&lt;/td&gt;
      &lt;td&gt;1313524&lt;/td&gt;
      &lt;td&gt;2400&lt;/td&gt;
      &lt;td&gt;2400&lt;/td&gt;
      &lt;td&gt;2400.0&lt;/td&gt;
      &lt;td&gt;36 months&lt;/td&gt;
      &lt;td&gt;15.96&lt;/td&gt;
      &lt;td&gt;84.33&lt;/td&gt;
      &lt;td&gt;C&lt;/td&gt;
      &lt;td&gt;C5&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;10.0&lt;/td&gt;
      &lt;td&gt;RENT&lt;/td&gt;
      &lt;td&gt;12252.0&lt;/td&gt;
      &lt;td&gt;Not Verified&lt;/td&gt;
      &lt;td&gt;Dec-2011&lt;/td&gt;
      &lt;td&gt;Fully Paid&lt;/td&gt;
      &lt;td&gt;n&lt;/td&gt;
      &lt;td&gt;small_business&lt;/td&gt;
      &lt;td&gt;606xx&lt;/td&gt;
      &lt;td&gt;IL&lt;/td&gt;
      &lt;td&gt;8.72&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;2.0&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;2.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;2956&lt;/td&gt;
      &lt;td&gt;98.5&lt;/td&gt;
      &lt;td&gt;10.0&lt;/td&gt;
      &lt;td&gt;f&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;3003.653644&lt;/td&gt;
      &lt;td&gt;3003.65&lt;/td&gt;
      &lt;td&gt;2400.00&lt;/td&gt;
      &lt;td&gt;603.65&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.00&lt;/td&gt;
      &lt;td&gt;0.00&lt;/td&gt;
      &lt;td&gt;649.91&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;10.0&lt;/td&gt;
      &lt;td&gt;603.653644&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;1071795&lt;/td&gt;
      &lt;td&gt;1306957&lt;/td&gt;
      &lt;td&gt;5600&lt;/td&gt;
      &lt;td&gt;5600&lt;/td&gt;
      &lt;td&gt;5600.0&lt;/td&gt;
      &lt;td&gt;60 months&lt;/td&gt;
      &lt;td&gt;21.28&lt;/td&gt;
      &lt;td&gt;152.39&lt;/td&gt;
      &lt;td&gt;F&lt;/td&gt;
      &lt;td&gt;F2&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;4.0&lt;/td&gt;
      &lt;td&gt;OWN&lt;/td&gt;
      &lt;td&gt;40000.0&lt;/td&gt;
      &lt;td&gt;Source Verified&lt;/td&gt;
      &lt;td&gt;Dec-2011&lt;/td&gt;
      &lt;td&gt;Charged Off&lt;/td&gt;
      &lt;td&gt;n&lt;/td&gt;
      &lt;td&gt;small_business&lt;/td&gt;
      &lt;td&gt;958xx&lt;/td&gt;
      &lt;td&gt;CA&lt;/td&gt;
      &lt;td&gt;5.55&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;2.0&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;11.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;5210&lt;/td&gt;
      &lt;td&gt;32.6&lt;/td&gt;
      &lt;td&gt;13.0&lt;/td&gt;
      &lt;td&gt;f&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;646.020000&lt;/td&gt;
      &lt;td&gt;646.02&lt;/td&gt;
      &lt;td&gt;162.02&lt;/td&gt;
      &lt;td&gt;294.94&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;189.06&lt;/td&gt;
      &lt;td&gt;2.09&lt;/td&gt;
      &lt;td&gt;152.39&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;7.0&lt;/td&gt;
      &lt;td&gt;-4953.980000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;1071570&lt;/td&gt;
      &lt;td&gt;1306721&lt;/td&gt;
      &lt;td&gt;5375&lt;/td&gt;
      &lt;td&gt;5375&lt;/td&gt;
      &lt;td&gt;5350.0&lt;/td&gt;
      &lt;td&gt;60 months&lt;/td&gt;
      &lt;td&gt;12.69&lt;/td&gt;
      &lt;td&gt;121.45&lt;/td&gt;
      &lt;td&gt;B&lt;/td&gt;
      &lt;td&gt;B5&lt;/td&gt;
      &lt;td&gt;Starbucks&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;RENT&lt;/td&gt;
      &lt;td&gt;15000.0&lt;/td&gt;
      &lt;td&gt;Verified&lt;/td&gt;
      &lt;td&gt;Dec-2011&lt;/td&gt;
      &lt;td&gt;Charged Off&lt;/td&gt;
      &lt;td&gt;n&lt;/td&gt;
      &lt;td&gt;other&lt;/td&gt;
      &lt;td&gt;774xx&lt;/td&gt;
      &lt;td&gt;TX&lt;/td&gt;
      &lt;td&gt;18.08&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;2.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;9279&lt;/td&gt;
      &lt;td&gt;36.5&lt;/td&gt;
      &lt;td&gt;3.0&lt;/td&gt;
      &lt;td&gt;f&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;1476.190000&lt;/td&gt;
      &lt;td&gt;1469.34&lt;/td&gt;
      &lt;td&gt;673.48&lt;/td&gt;
      &lt;td&gt;533.42&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;269.29&lt;/td&gt;
      &lt;td&gt;2.52&lt;/td&gt;
      &lt;td&gt;121.45&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;7.0&lt;/td&gt;
      &lt;td&gt;-3898.810000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;1070078&lt;/td&gt;
      &lt;td&gt;1305201&lt;/td&gt;
      &lt;td&gt;6500&lt;/td&gt;
      &lt;td&gt;6500&lt;/td&gt;
      &lt;td&gt;6500.0&lt;/td&gt;
      &lt;td&gt;60 months&lt;/td&gt;
      &lt;td&gt;14.65&lt;/td&gt;
      &lt;td&gt;153.45&lt;/td&gt;
      &lt;td&gt;C&lt;/td&gt;
      &lt;td&gt;C3&lt;/td&gt;
      &lt;td&gt;Southwest Rural metro&lt;/td&gt;
      &lt;td&gt;5.0&lt;/td&gt;
      &lt;td&gt;OWN&lt;/td&gt;
      &lt;td&gt;72000.0&lt;/td&gt;
      &lt;td&gt;Not Verified&lt;/td&gt;
      &lt;td&gt;Dec-2011&lt;/td&gt;
      &lt;td&gt;Fully Paid&lt;/td&gt;
      &lt;td&gt;n&lt;/td&gt;
      &lt;td&gt;debt_consolidation&lt;/td&gt;
      &lt;td&gt;853xx&lt;/td&gt;
      &lt;td&gt;AZ&lt;/td&gt;
      &lt;td&gt;16.12&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;2.0&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;14.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;4032&lt;/td&gt;
      &lt;td&gt;20.6&lt;/td&gt;
      &lt;td&gt;23.0&lt;/td&gt;
      &lt;td&gt;f&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;7677.520000&lt;/td&gt;
      &lt;td&gt;7677.52&lt;/td&gt;
      &lt;td&gt;6500.00&lt;/td&gt;
      &lt;td&gt;1177.52&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.00&lt;/td&gt;
      &lt;td&gt;0.00&lt;/td&gt;
      &lt;td&gt;1655.54&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;13.0&lt;/td&gt;
      &lt;td&gt;1177.520000&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;p&gt;Get a list of all the columns. Later we will remove the columns we are not using and identify which colums are categorical and which are numeric.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;all_columns&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lending_df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;columns&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tolist&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Remove some of the variables that indicate a bad loan and would represent &lt;a href=&quot;https://en.wikipedia.org/wiki/Leakage_(machine_learning)&quot;&gt;&lt;strong&gt;target leakage&lt;/strong&gt;&lt;/a&gt;. These columns are in the data set because Lending Club only provides a current snapshot of their loans. To avoid the leakage problem we ideally want time-stamped transactions within a relational database.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;to_remove&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'issue_d'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'emp_title'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'zip_code'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'earned'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'total_rec_prncp'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'recoveries'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'total_rec_int'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
             &lt;span class=&quot;s&quot;&gt;'total_rec_late_fee'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'collection_recovery_fee'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'next_pyment_d'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'loan_status'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'pymnt_plan'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
             &lt;span class=&quot;s&quot;&gt;'id'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'member_id'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'out_prncp'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'out_prncp_inv'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'total_pymnt'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'total_pymnt_inv'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'last_pymnt_amnt'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
             &lt;span class=&quot;s&quot;&gt;'next_pymnt_d'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'collections_12_mths_ex_med'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'mths_since_last_major_derog'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'policy_code'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
             &lt;span class=&quot;s&quot;&gt;'credit_length_in_years'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Identify the columns that should be converted from string to categorical.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;cat_features&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'term'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'grade'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'sub_grade'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'home_ownership'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                      &lt;span class=&quot;s&quot;&gt;'is_inc_v'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'purpose'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'addr_state'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                      &lt;span class=&quot;s&quot;&gt;'initial_list_status'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The target column is &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;bad_loan&lt;/code&gt;. It takes values of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;1&lt;/code&gt; for a bad loan and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;0&lt;/code&gt; for one which has not gone bad (or not gone bad &lt;em&gt;yet&lt;/em&gt; in some cases).&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;target&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'bad_loan'&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;to_remove&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;target&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Get a list of all the predictors and numeric features.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;predictors&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;s&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;all_columns&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;s&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;not&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;to_remove&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;num_features&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;s&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;predictors&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;s&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;not&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cat_features&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Check the shape of the data so we have an idea of how many rows we have to train and test our classifier with.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;lending_df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;(36842, 49)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We have 36,842 rows to play with.&lt;/p&gt;

&lt;h3 id=&quot;split-into-training-and-test-sets&quot;&gt;Split into training and test sets&lt;/h3&gt;

&lt;p&gt;We will train against a separate data set from the one we will test our trained classifier against, using scikit-learn’s &lt;a href=&quot;https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;train_test_split&lt;/code&gt;&lt;/a&gt; to partition the data set randomly.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn.model_selection&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;train_test_split&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;I am going to fix the test size at 5,000; this seems like a good enough number while retaining plenty of sample to train the classifier.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; I like to specify parameters as keyword arguments. This way we can load configuration files if we want when we run from a script without having to edit the code. This is much better than having to changing values within our script for a simple change.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# It would be quite easy to stick these in a file and read them in
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;split_config&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;dict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;random_state&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1737&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;test_size&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;5000&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;X_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_test&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;train_test_split&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lending_df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;predictors&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lending_df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;target&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;split_config&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Check that we get the right number of rows back in the test set. I like to check things are as they should be (at least in my mind) so I write tests.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;assert&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;split_config&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'test_size'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Examine the distribution of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;bad_loans&lt;/code&gt; in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;y_train&lt;/code&gt;.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;y_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;value_counts&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;0    26693
1     5149
Name: bad_loan, dtype: int64
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;build-a-simple-classifier&quot;&gt;Build a simple classifier&lt;/h2&gt;

&lt;p&gt;For this example I will use scikit-learn’s &lt;a href=&quot;https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;GradientBoostingClassifier&lt;/code&gt;&lt;/a&gt;. I shall also use the excellent &lt;a href=&quot;https://scikit-learn.org/stable/modules/compose.html#combining-estimators&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Pipeline&lt;/code&gt;&lt;/a&gt; capability of scikit-learn to build our classifier. This allows us to combine our preprocessing and classifier into the one composite classifier object. There are many good reasons to do this that I will not go into further here.&lt;/p&gt;

&lt;p&gt;One I will mention is that we cab perform a search over preprocessing parameters if we like, though I am not doing that here.&lt;/p&gt;

&lt;p&gt;To encode the categorical variables, we will use &lt;a href=&quot;http://contrib.scikit-learn.org/category_encoders/catboost.html&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;CatBoostEncoder&lt;/code&gt;&lt;/a&gt; from &lt;a href=&quot;http://contrib.scikit-learn.org/category_encoders/&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;category_encoders&lt;/code&gt;&lt;/a&gt;, which you can install using &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pip&lt;/code&gt;.&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;pip &lt;span class=&quot;nb&quot;&gt;install &lt;/span&gt;category_encoders
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn.ensemble&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;GradientBoostingClassifier&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn.pipeline&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Pipeline&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn.compose&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ColumnTransformer&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn.metrics&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;roc_auc_score&lt;/span&gt;

&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;category_encoders&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ce&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn.impute&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;SimpleImputer&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;set-up-the-pipelines&quot;&gt;Set up the pipelines&lt;/h3&gt;

&lt;p&gt;The pipeline we will set up has two steps.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;The preprocessor, which will use &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;CatBoostEncoder&lt;/code&gt; for categorical features and &lt;a href=&quot;https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;SimpleImputer&lt;/code&gt;&lt;/a&gt; for numeric
features.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;A &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;GradientBoostingClassifier&lt;/code&gt; as the model.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;We combine the two preprocessors using &lt;a href=&quot;https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ColumnTransformer&lt;/code&gt;&lt;/a&gt;, which uses takes a tuple of the format &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;(name, transformer, columns)&lt;/code&gt;.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;preprocessor&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ColumnTransformer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;transformers&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'categorical'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ce&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;CatBoostEncoder&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cat_features&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'numeric'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;SimpleImputer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_features&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;check_array&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;preprocessor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit_transform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;assert&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;check_array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;predictors&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;feature_names&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cat_features&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_features&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Check whether &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;check_array&lt;/code&gt; contains &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NaN&lt;/code&gt;s.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;assert&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;not&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;isnan&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;check_array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;any&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Create and test our classifier. Remember, this will become a step in our pipeline.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;gbm&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;GradientBoostingClassifier&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Here I test if the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;fit&lt;/code&gt; method runs using a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;try … except&lt;/code&gt; construct.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;try&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;gbm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;check_array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;except&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;assert&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'Bad gbm fit'&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Now let us create a pipeline that includes our GBM. We name each step of the pipeline so we can refer to parameters in each of the steps.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;gbm_pipeline&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Pipeline&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'preprocessor'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;preprocessor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'gbm'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;gbm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We can refer to parameters in a pipeline by using the format &lt;strong&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;name&lt;/code&gt;&lt;/strong&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;__&lt;/code&gt;&lt;strong&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;parameter&lt;/code&gt;&lt;/strong&gt;.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;param_grid&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;'gbm__n_estimators'&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;20&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;'gbm__max_depth'&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;'gbm__min_samples_leaf'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;30&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;'gbm__learning_rate'&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.15&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;'gbm__subsample'&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1.0&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;To set a parameter for a one-off fit, we use the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;set_params&lt;/code&gt; method. Here we are setting the parameters to get a pretty-good model using default parameters. We will compare the results from this with those from successive halving later.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;gbm_pipeline&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;set_params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;param_grid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Pipeline(steps=[('preprocessor',
                 ColumnTransformer(transformers=[('categorical',
                                                  CatBoostEncoder(),
                                                  ['term', 'grade', 'sub_grade',
                                                   'home_ownership', 'is_inc_v',
                                                   'purpose', 'addr_state',
                                                   'initial_list_status']),
                                                 ('numeric', SimpleImputer(),
                                                  ['loan_amnt', 'funded_amnt',
                                                   'funded_amnt_inv',
                                                   'int_rate', 'installment',
                                                   'emp_length', 'annual_inc',
                                                   'dti', 'delinq_2yrs',
                                                   'inq_last_6mths',
                                                   'mths_since_last_delinq',
                                                   'mths_since_last_record',
                                                   'open_acc', 'pub_rec',
                                                   'revol_bal', 'revol_util',
                                                   'total_acc'])])),
                ('gbm',
                 GradientBoostingClassifier(learning_rate=0.15, max_depth=1,
                                            min_samples_leaf=30,
                                            n_estimators=20))])
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Test that it runs.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;try&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;gbm_pipeline&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;except&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;assert&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'Bad pipeline fit'&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;check-the-performance-of-the-basic-model&quot;&gt;Check the performance of the basic model&lt;/h3&gt;

&lt;p&gt;To give a baseline of the performance of our basic model that we fit in the previous step, we calculate the &lt;a href=&quot;https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc&quot;&gt;area under the receiver operating characteristic curve (ROC)&lt;/a&gt;.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;y_pred&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;gbm_pipeline&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;predict_proba&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)[:,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;baseline_auc&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;roc_auc_score&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_pred&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;f'The baseline AUC performance is &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;baseline_auc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:.&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;.'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;The baseline AUC performance is 0.696.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This model is one we expect to be okay, if not spectacular.&lt;/p&gt;

&lt;h2 id=&quot;test-successive-halving&quot;&gt;Test successive halving&lt;/h2&gt;

&lt;p&gt;Successive halving is still an experimental feature, so we need to turn it on explicitly.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# explicitly require this experimental feature
&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn.experimental&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;enable_halving_search_cv&lt;/span&gt;  

&lt;span class=&quot;c1&quot;&gt;# now you can import normally from model_selection
&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn.model_selection&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;HalvingGridSearchCV&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn.model_selection&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;HalvingRandomSearchCV&lt;/span&gt;

&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;scipy&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Check how many rows we have in our training set.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;X_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;31842
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We have 31,842 rows in our training data set, so we will set the starting number of minimum resources to 256. This will double at each iteration until we hit the limit.&lt;/p&gt;

&lt;p&gt;Set the parameter grid for the search.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;param_grid&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;'gbm__n_estimators'&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;scipy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stats&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randint&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;20&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2001&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;'gbm__n_iter_no_change'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;50&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;'gbm__max_depth'&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;scipy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stats&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randint&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;13&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;'gbm__min_samples_leaf'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;scipy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stats&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randint&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;51&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;'gbm__learning_rate'&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;scipy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stats&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;uniform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.01&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;'gbm__subsample'&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Set some configurable parameters for the &lt;a href=&quot;https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.HalvingRandomSearchCV.html&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;HalvingRandomSearchCV&lt;/code&gt;&lt;/a&gt; we will run. We will use the number of samples as the resource and use AUC (area under the curve) to evaluate the best model.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# config
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hrs_params&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;dict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;verbose&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cv&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;factor&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;resource&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'n_samples'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;min_resources&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;256&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;scoring&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'roc_auc'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                  &lt;span class=&quot;n&quot;&gt;random_state&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1707&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;I like to note how much time potentially long-running cells take to execute. Do this easily in Jupyter with the &lt;a href=&quot;https://ipython.readthedocs.io/en/stable/interactive/magics.html&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;%%time&lt;/code&gt; cell magic&lt;/a&gt;.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;o&quot;&gt;%%&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;time&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;seed&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hrs_params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'random_state'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;sh&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;HalvingRandomSearchCV&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;gbm_pipeline&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;param_grid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hrs_params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;sh&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;n_iterations: 7
n_required_iterations: 7
n_possible_iterations: 7
min_resources_: 256
max_resources_: 31842
aggressive_elimination: False
factor: 2
----------
iter: 0
n_candidates: 124
n_resources: 256
Fitting 5 folds for each of 124 candidates, totalling 620 fits
----------
iter: 1
n_candidates: 62
n_resources: 512
Fitting 5 folds for each of 62 candidates, totalling 310 fits
----------
iter: 2
n_candidates: 31
n_resources: 1024
Fitting 5 folds for each of 31 candidates, totalling 155 fits
----------
iter: 3
n_candidates: 16
n_resources: 2048
Fitting 5 folds for each of 16 candidates, totalling 80 fits
----------
iter: 4
n_candidates: 8
n_resources: 4096
Fitting 5 folds for each of 8 candidates, totalling 40 fits
----------
iter: 5
n_candidates: 4
n_resources: 8192
Fitting 5 folds for each of 4 candidates, totalling 20 fits
----------
iter: 6
n_candidates: 2
n_resources: 16384
Fitting 5 folds for each of 2 candidates, totalling 10 fits
CPU times: user 7min 40s, sys: 1.19 s, total: 7min 41s
Wall time: 7min 42s





HalvingRandomSearchCV(estimator=Pipeline(steps=[('preprocessor',
                                                 ColumnTransformer(transformers=[('categorical',
                                                                                  CatBoostEncoder(),
                                                                                  ['term',
                                                                                   'grade',
                                                                                   'sub_grade',
                                                                                   'home_ownership',
                                                                                   'is_inc_v',
                                                                                   'purpose',
                                                                                   'addr_state',
                                                                                   'initial_list_status']),
                                                                                 ('numeric',
                                                                                  SimpleImputer(),
                                                                                  ['loan_amnt',
                                                                                   'funded_amnt',
                                                                                   'funded_amnt_inv',
                                                                                   'int_rate',
                                                                                   'installment',
                                                                                   'emp_length',
                                                                                   'annual_...
                                           'gbm__max_depth': &amp;lt;scipy.stats._distn_infrastructure.rv_frozen object at 0x7fa3353f3910&amp;gt;,
                                           'gbm__min_samples_leaf': &amp;lt;scipy.stats._distn_infrastructure.rv_frozen object at 0x7fa3354bad00&amp;gt;,
                                           'gbm__n_estimators': &amp;lt;scipy.stats._distn_infrastructure.rv_frozen object at 0x7fa3354ba910&amp;gt;,
                                           'gbm__n_iter_no_change': [50],
                                           'gbm__subsample': [1.0]},
                      random_state=1707,
                      refit=&amp;lt;function _refit_callable at 0x7fa3335f0280&amp;gt;,
                      scoring='roc_auc', verbose=1)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This took 6 iterations and around 8 minutes on my machine.&lt;/p&gt;

&lt;p&gt;Note that at each step the number of candidates halved until we got to two in the final step.&lt;/p&gt;

&lt;p&gt;Once successive halving has settled on a final candidate, it fits it using the whole sample.&lt;/p&gt;

&lt;p&gt;Examine the parameters of the best and final estimator.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;sh&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;best_estimator_&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Pipeline(steps=[('preprocessor',
                 ColumnTransformer(transformers=[('categorical',
                                                  CatBoostEncoder(),
                                                  ['term', 'grade', 'sub_grade',
                                                   'home_ownership', 'is_inc_v',
                                                   'purpose', 'addr_state',
                                                   'initial_list_status']),
                                                 ('numeric', SimpleImputer(),
                                                  ['loan_amnt', 'funded_amnt',
                                                   'funded_amnt_inv',
                                                   'int_rate', 'installment',
                                                   'emp_length', 'annual_inc',
                                                   'dti', 'delinq_2yrs',
                                                   'inq_last_6mths',
                                                   'mths_since_last_delinq',
                                                   'mths_since_last_record',
                                                   'open_acc', 'pub_rec',
                                                   'revol_bal', 'revol_util',
                                                   'total_acc'])])),
                ('gbm',
                 GradientBoostingClassifier(learning_rate=0.37976865555958783,
                                            max_depth=1, min_samples_leaf=16,
                                            n_estimators=1500,
                                            n_iter_no_change=50))])
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We can check how the scoring progressed in each iteration. Recall our baseline AUC score is 0.696.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;res_df&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;DataFrame&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sh&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cv_results_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;res_df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;groupby&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'iter'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'mean_test_score'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;max&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;iter
0    0.618091
1    0.652617
2    0.648898
3    0.695115
4    0.717969
5    0.725628
6    0.727079
Name: mean_test_score, dtype: float64
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;According to the cross-validation AUC score on the &lt;em&gt;training&lt;/em&gt; set, successive halving produced a better model than the naive GBM after iteration 4.&lt;/p&gt;

&lt;p&gt;Check the performance of the final model against the test set. This will show us whether the performance has improved compared with the initial baseline model we fit against the same test set.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;preds_test&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sh&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;best_estimator_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;predict_proba&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)[:,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;baseline_auc&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;roc_auc_score&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_pred&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;rsh_auc&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;roc_auc_score&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;preds_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;rsh_auc&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;0.7233991664802961
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We got an AUC score of 0.723 which is better than 0.696 — a definite improvement.&lt;/p&gt;

&lt;h1 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h1&gt;

&lt;p&gt;The successive halving approach is a reasonable algorithm that does the same sort of thing that I advise learning data scientists to do. That is, perform searches using smaller samples at first to cut down processing time. Once we have a good idea of the best parameters, fit against a large sample to improve the precision of the estimators.&lt;/p&gt;

&lt;p&gt;Of course, one could argue the position that the parameters of the successive halving are additional hyperparameters that need to be optimised on. But that would send you down the &lt;strong&gt;rabbit hole&lt;/strong&gt; and distract you from this fact: for most purposes, a good model is not significantly better than the best model. Your time is usually better spent ensuring the entire problem solving process, end-to-end pipeline and experimental design are also ‘good enough’.&lt;/p&gt;</content><author><name></name></author><category term="machine-learning" /><category term="sklearn" /><summary type="html">New features in scikit-learn 0.24</summary></entry><entry><title type="html">The case for Markdown everywhere</title><link href="http://www.3crownsconsulting.com.au/markdown/2020/12/08/the-case-for-markdown-everywhere.html" rel="alternate" type="text/html" title="The case for Markdown everywhere" /><published>2020-12-08T22:00:00+00:00</published><updated>2020-12-08T22:00:00+00:00</updated><id>http://www.3crownsconsulting.com.au/markdown/2020/12/08/the-case-for-markdown-everywhere</id><content type="html" xml:base="http://www.3crownsconsulting.com.au/markdown/2020/12/08/the-case-for-markdown-everywhere.html">&lt;h1 id=&quot;why-i-love-markdown-and-want-to-see-its-use-become-more-widespread&quot;&gt;&lt;em&gt;Why I love Markdown and want to see its use become more widespread&lt;/em&gt;&lt;/h1&gt;

&lt;h2 id=&quot;intro&quot;&gt;Intro&lt;/h2&gt;

&lt;p&gt;I &lt;em&gt;love&lt;/em&gt; writing in &lt;a href=&quot;https://en.wikipedia.org/wiki/Markdown&quot;&gt;Markdown&lt;/a&gt;. It is simple to write, easy to remember and is widely adopted. When writing in Markdown, all I need to concentrate on is the writing and the keyboard. Markdown files are readable as text: everybody understands what you mean when you surround a word by asterisks — like **really**.&lt;/p&gt;

&lt;p&gt;In this article, I go into why I like Markdown and why I would like to see support for it in all applications.&lt;/p&gt;

&lt;p&gt;$$  $$&lt;/p&gt;

&lt;h2 id=&quot;what-it-is-and-a-brief-history&quot;&gt;What it is and a brief history&lt;/h2&gt;

&lt;p&gt;Does anyone remember old word processing applications (or programs) from the 1980s that ran on &lt;a href=&quot;https://en.wikipedia.org/wiki/MS-DOS&quot;&gt;MS-DOS&lt;/a&gt; like &lt;a href=&quot;https://en.wikipedia.org/wiki/WordStar&quot;&gt;WordStar&lt;/a&gt; and &lt;a href=&quot;https://www.wordperfect.com/en/&quot;&gt;WordPerfect&lt;/a&gt;? Before the dominance of &lt;a href=&quot;https://en.wikipedia.org/wiki/Microsoft_Word&quot;&gt;Microsoft Word&lt;/a&gt; and &lt;a href=&quot;https://en.wikipedia.org/wiki/WYSIWYG&quot;&gt;WYSIWYG&lt;/a&gt; on &lt;a href=&quot;https://en.wikipedia.org/wiki/Microsoft_Windows&quot;&gt;Microsoft Windows&lt;/a&gt; became the &lt;em&gt;de facto&lt;/em&gt; standard for document writing?&lt;/p&gt;

&lt;p&gt;You wrote the document using special characters for formatting. If you wanted to preview the final product, you had to render the document to print or a special preview screen. But as long as you knew the characters to type, it was remarkably quick and efficient to compose a document. You were not stuck in the constant hell of switching between keyboard and mouse to compose and format a document.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://upload.wikimedia.org/wikipedia/en/e/e3/Wordstar_Screenshot.png&quot; alt=&quot;Some of the special characters appearing in a WordStar document on MS-DOS&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Some of the special characters appearing in a WordStar document on MS-DOS.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://upload.wikimedia.org/wikipedia/commons/0/00/Compaq_Portable_and_Wordperfect.JPG&quot; alt=&quot;WordPerfect was perfect for word processing when computers looked like Archer technology&quot; style=&quot;zoom:80%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;WordPerfect was perfect for word processing when computers looked like &lt;a href=&quot;https://en.wikipedia.org/wiki/Archer_%282009_TV_series%29&quot;&gt;Archer&lt;/a&gt; technology.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Just while I am digressing … I recall from my childhood that word processors used to be actual computers themselves — computers that ran only one application. Offices had banks of typists who had upgraded from electric typewriters to word processors. I think the old equipment looks great now in a retro chic way.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://i.pinimg.com/originals/28/74/50/2874505128a2430de587030671d6d413.jpg&quot; alt=&quot;A Wang word processor really does have that Archer vibe&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;A Wang word processor really does have that Archer vibe.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://cdn.newsday.com/polopoly_fs/1.5178733.1461864381!/httpImage/image.jpg_gen/derivatives/display_960/image.jpg&quot; alt=&quot;The computers from Archer feature state of the art mechanical keyboards&quot; style=&quot;zoom:80%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;The computers from Archer feature state of the art mechanical keyboards.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;According to the &lt;a href=&quot;https://en.wikipedia.org/wiki/Markdown&quot;&gt;Internet&lt;/a&gt;, Markdown was created in 2004 by &lt;a href=&quot;https://en.wikipedia.org/wiki/John_Gruber&quot;&gt;John Gruber&lt;/a&gt; and the late, troubled &lt;a href=&quot;https://en.wikipedia.org/wiki/Aaron_Swartz&quot;&gt;Aaron Swartz&lt;/a&gt;. The aim: a text format that could convert easily to valid &lt;a href=&quot;https://en.wikipedia.org/wiki/HTML&quot;&gt;HTML&lt;/a&gt; or &lt;a href=&quot;https://en.wikipedia.org/wiki/XHTML&quot;&gt;XHTML&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The name ‘Markdown’ is a simple play on the term ‘markup’. &lt;a href=&quot;https://en.wikipedia.org/wiki/Markup_language&quot;&gt;‘Markup’&lt;/a&gt; refers to a language that describes a document’s formatting separately from its content. This makes it straightforward to change styles. Examples of popular markup languages are &lt;a href=&quot;https://en.wikipedia.org/wiki/HTML&quot;&gt;HTML&lt;/a&gt; and &lt;a href=&quot;https://en.wikipedia.org/wiki/LaTeX&quot;&gt;LaTeX&lt;/a&gt; ($\LaTeX$).&lt;/p&gt;

&lt;p&gt;Both of these languages have not achieved their full potential as markup languages, mainly because they have evolved to find widespread use in their specific niches beyond a simple requirement for document processing.&lt;/p&gt;

&lt;h2 id=&quot;where-can-i-use-it-and-what-is-it-good-for&quot;&gt;Where can I use it and what is it good for?&lt;/h2&gt;

&lt;p&gt;Markdown has risen to prominence largely through its use on &lt;a href=&quot;github.com&quot;&gt;GitHub&lt;/a&gt;, where is used for README files. &lt;a href=&quot;https://github.github.com/gfm/&quot;&gt;Github has its own flavour&lt;/a&gt; and has &lt;a href=&quot;https://guides.github.com/features/mastering-markdown/&quot;&gt;extensive guides&lt;/a&gt; to help learn how best to use it. But learning Markdown is not that hard!&lt;/p&gt;

&lt;p&gt;The basics of using Markdown are straightforward.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Use &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;#&lt;/code&gt; for heading level 1, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;##&lt;/code&gt; heading level 2 and so one up to level 6 (please do not use this — I cannot keep your overcomplicated document structure in my head)&lt;/li&gt;
  &lt;li&gt;Surround text to be displayed in &lt;em&gt;italics&lt;/em&gt; by a single underscore (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;_italics_&lt;/code&gt;)&lt;/li&gt;
  &lt;li&gt;Similarly, use two asterisks (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;**bold**&lt;/code&gt;) for &lt;strong&gt;bold&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;Bullet points are created using two spaces and an asterisk&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-markdown highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;p&quot;&gt;  *&lt;/span&gt; bullet point 1
&lt;span class=&quot;p&quot;&gt;  *&lt;/span&gt; bullet point 2
&lt;span class=&quot;p&quot;&gt;    -&lt;/span&gt; subpoint 2.1
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;Numbered lists use a number followed by a full stop — Markdown will ignore the actual number you type, however&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-markdown highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;p&quot;&gt;1.&lt;/span&gt; Point 1
&lt;span class=&quot;p&quot;&gt;3.&lt;/span&gt; Point 2
&lt;span class=&quot;p&quot;&gt;3.&lt;/span&gt; Point 3
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;Unformatted text is specified using a backquote `; you can format blocks of text using matching sets of triple backquotes (```)&lt;/li&gt;
  &lt;li&gt;Block quotes of text are specified using a greater-than sign preceding the text&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-markdown highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;gt&quot;&gt;&amp;gt; This is how you do a block quote&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;hr /&gt;

&lt;blockquote&gt;
  &lt;p&gt;This is how you do a block quote&lt;/p&gt;
&lt;/blockquote&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;analytics-and-maths&quot;&gt;Analytics and maths&lt;/h3&gt;

&lt;p&gt;The popular analytical tools &lt;a href=&quot;https://jupyter.org&quot;&gt;Jupyter&lt;/a&gt; and &lt;a href=&quot;https://rstudio.com&quot;&gt;RStudio&lt;/a&gt; both use Markdown for writing reports that can include code, calculations, charts and other enhancements.&lt;/p&gt;

&lt;p&gt;The other killer feature for someone like me is many versions support mathematics in the form of &lt;a href=&quot;https://en.wikipedia.org/wiki/LaTeX&quot;&gt;LaTeX&lt;/a&gt; commands.
So it is easy to include a maths equation like $ y = X\beta + \epsilon $.&lt;/p&gt;

&lt;h2 id=&quot;why-i-like-it&quot;&gt;Why I like it&lt;/h2&gt;

&lt;h3 id=&quot;structured-documents&quot;&gt;Structured documents&lt;/h3&gt;

&lt;p&gt;Using Markdown, or even markup languages, it is easy to create structured documents and &lt;em&gt;much harder&lt;/em&gt; to hard-code document-specific formatting than in &lt;a href=&quot;https://en.wikipedia.org/wiki/Microsoft_Word&quot;&gt;Word&lt;/a&gt;. This can be both a good and bad thing; I like it because it forces a writer to focus on the content rather than how the document looks.&lt;/p&gt;

&lt;h3 id=&quot;workflow&quot;&gt;Workflow&lt;/h3&gt;

&lt;p&gt;I also find it much more intuitive and less disruptive to my workflow to include any formatting as characters while I type. Unlike &lt;a href=&quot;https://en.wikipedia.org/wiki/Microsoft_Word&quot;&gt;MS Word&lt;/a&gt;, I am not jumping around between the keyboard and the mouse for formatting. This becomes even more important when you are working from a laptop and navigating between its keyboard and trackpad.&lt;/p&gt;

&lt;p&gt;For those series writers, some software tools allow have a &lt;strong&gt;focus mode&lt;/strong&gt; to help you concentrate. &lt;a href=&quot;https://lifehacker.com/i-still-use-plain-text-for-everything-and-i-love-it-1758380840&quot;&gt;Many old-school writers prefer writing using text editors only.&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&quot;separation-of-text-and-formatting&quot;&gt;Separation of text and formatting&lt;/h3&gt;

&lt;p&gt;Markdown describes the context and the structure of the formatting only, unlike with word processors like &lt;a href=&quot;https://en.wikipedia.org/wiki/Microsoft_Word&quot;&gt;MS Word&lt;/a&gt;, where the formatting is enmeshed in the document itself. The separation makes it easy to change styles. &lt;a href=&quot;https://en.wikipedia.org/wiki/Separation_of_content_and_presentation&quot;&gt;This is in line with good design practices.&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/Text-and-style-together.svg&quot; alt=&quot;Text and style confounded&quot; style=&quot;zoom:80%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Microsoft_Word&quot;&gt;MS Word&lt;/a&gt; combines text and style in the same document.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/Text-and-style-separate.svg&quot; alt=&quot;Text and style separate&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Good design dictates that content formatting and style should be separate.&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&quot;markdown-files-are-plain-text&quot;&gt;Markdown files are plain text&lt;/h3&gt;

&lt;p&gt;Markdown files are text. This makes them &lt;strong&gt;extremely portable&lt;/strong&gt; and editable with any text editor. But of course there are specialist tools that give you a bit more. Two of my favourites are &lt;a href=&quot;https://typora.io&quot;&gt;&lt;strong&gt;Typora&lt;/strong&gt;&lt;/a&gt; and  &lt;a href=&quot;https://stackedit.io&quot;&gt;StackEdit&lt;/a&gt;. Typora is available for Linux, Mac OS and Windows. StackEdit runs in the browser and integrates with your online storage account.&lt;/p&gt;

&lt;h3 id=&quot;support-for-html&quot;&gt;Support for HTML&lt;/h3&gt;

&lt;p&gt;Markdown replaces the HTML characters &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&amp;lt;&lt;/code&gt; &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&amp;gt;&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&amp;amp;&lt;/code&gt; with character entity references. This means you can do neat things like this: this text is &lt;span style=&quot;color:blue&quot;&gt;blue&lt;/span&gt; (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&amp;lt;span style='color:blue'&amp;gt;blue&amp;lt;/span&amp;gt;&lt;/code&gt;) or ½ (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&amp;amp;frac12;&lt;/code&gt;) . HTML support is very useful for training documentation and writing technical manuals, for example, to type things like &lt;kbd&gt;Enter&lt;/kbd&gt; (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&amp;lt;kbd&amp;gt;Enter&amp;lt;/kbd&amp;gt;&lt;/code&gt;). Because it resolves to HTML, it is also great for designing simple websites using tools like &lt;a href=&quot;https://jekyllrb.com&quot;&gt;Jekyll&lt;/a&gt;. Using HTML makes Markdown &lt;em&gt;a little&lt;/em&gt; less usable, however.&lt;/p&gt;

&lt;h3 id=&quot;writing-code-blocks&quot;&gt;Writing code blocks&lt;/h3&gt;

&lt;p&gt;Markdown is also great for writing code; it supports a whole bunch of languages. Here is an example of how it renders  Python code.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;add_one&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
  &lt;span class=&quot;c1&quot;&gt;# return the result
&lt;/span&gt;  &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;em&gt;Example of a basic Python function rendered in Markdown.&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&quot;presentation-slides&quot;&gt;Presentation slides&lt;/h3&gt;

&lt;p&gt;There are tools and frameworks that let you write Markdown to generate a series of slides and presentations. These can be pretty good and highly recommended when you need to include code in your presentation. The downside is that where you want diagrams and images, it seems less than natural to include them in Markdown — you pretty much end up with a frequent write–display–review workflow. Note though, that this mode of work is &lt;em&gt;very&lt;/em&gt; different from composing presentations in &lt;a href=&quot;https://www.microsoft.com/en-us/microsoft-365/powerpoint&quot;&gt;PowerPoint&lt;/a&gt; or &lt;a href=&quot;https://prezi.com&quot;&gt;Prezi&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;My favourite way of doing this is to use &lt;a href=&quot;https://rstudio.com&quot;&gt;RStudio&lt;/a&gt; to create &lt;a href=&quot;https://bookdown.org/yihui/rmarkdown/revealjs.html&quot;&gt;revealjs&lt;/a&gt; presentations.&lt;/p&gt;

&lt;h2 id=&quot;what-could-make-it-better&quot;&gt;What could make it better&lt;/h2&gt;

&lt;p&gt;So you get that I love Markdown. But where could it be better? Improved support for tables, styles and images could improve Markdown, but I do not have great suggestions on how to design the improvements I would like to see.&lt;/p&gt;

&lt;h3 id=&quot;tables&quot;&gt;Tables&lt;/h3&gt;

&lt;p&gt;Writing tables in Markdown is okay. You basically write a text-based table like this.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;| fruit  | colour | rating |
|--------|--------|-------:|
| banana | yellow | 10     |
| orange | orange |  5     |
| lemon  | yellow |  2     |
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;You do not even need to align all the heading and columns if you are lazy.&lt;/p&gt;

&lt;p&gt;It will display as a nicely formatted table like this:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;fruit&lt;/th&gt;
      &lt;th&gt;colour&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;rating&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;banana&lt;/td&gt;
      &lt;td&gt;yellow&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;10&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;orange&lt;/td&gt;
      &lt;td&gt;orange&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;5&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;lemon&lt;/td&gt;
      &lt;td&gt;yellow&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;2&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;I do like this — a lot. But sometimes you need detailed tables with multiple rows in a cell or merged cells. Markdown cannot cope with this natively. The workaround is to write the table in HTML, but it is painful (unless you use a helper tool) and the raw text is no longer readable.&lt;/p&gt;

&lt;h3 id=&quot;css&quot;&gt;CSS&lt;/h3&gt;

&lt;p&gt;CSS can be used to provide custom styling and even some actions that control HTML. Without a good CSS tool, much of this is alien to me (being a simple user), so I am stuck with what we get from the tools as they convert the Markdown syntax to a display format. But some of you will be more capable than I am of combining the CSS with the Markdown converter to create nice looking outputs.&lt;/p&gt;

&lt;h3 id=&quot;images&quot;&gt;Images&lt;/h3&gt;

&lt;p&gt;Image handling can be fiddly. If you want to include images as they are, it is straightforward — &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;![image text](image location)&lt;/code&gt;. But to caption or zoom or anything else that might be nice, you need to go into HTML and use &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&amp;lt;img&amp;gt;&lt;/code&gt; tags. Again this messes with the readability of the raw text.&lt;/p&gt;

&lt;p&gt;The other thing I do not like about images is how they are handled in the rendered documents. In some implementations of Markdown to HTML (or other destinations), the hard-coded links to images are maintained. This does not make a lot of sense for local files. You essentially need to load these to a destination that is accessible wherever the document is viewed.&lt;/p&gt;

&lt;h2 id=&quot;what-would-make-it-worse&quot;&gt;What would make it worse&lt;/h2&gt;

&lt;p&gt;What makes Markdown great for writing is its simplicity, its portability and its readability. If any extensions to the language change this, it will be for the worse.&lt;/p&gt;

&lt;h3 id=&quot;portability&quot;&gt;Portability&lt;/h3&gt;

&lt;p&gt;One of the enhancements that could reduce its portability is fragmentation into too many variants each with their own syntax. There are already several versions of Markdown in the world: two are &lt;a href=&quot;https://github.github.com/gfm/&quot;&gt;GitHub-Flavored Markdown&lt;/a&gt; and &lt;a href=&quot;https://michelf.ca/projects/php-markdown/extra/&quot;&gt;PHP Markdown Extra&lt;/a&gt; (which adds additional features not available in standard Markdown). If there are two many variants of Markdown its portability is reduced. You will not have a reliable way of knowing if a particular Markdown file will display correctly in another variant.&lt;/p&gt;

&lt;h3 id=&quot;simplicity-and-readability&quot;&gt;Simplicity and readability&lt;/h3&gt;

&lt;p&gt;What makes Markdown simple compared to work is its limitations. The temptation will be to keep adding things to it: additional style elements, diagrams, improvements and so on. Additional style elements like CSS encroaching into the Markdown syntax reduces it simplicity &lt;em&gt;and&lt;/em&gt; the separate of content and style.&lt;/p&gt;

&lt;p&gt;Adding diagrams (such as &lt;a href=&quot;https://mermaid-js.github.io/mermaid/#/&quot;&gt;Mermaid&lt;/a&gt; included in &lt;a href=&quot;typora&quot;&gt;Typora&lt;/a&gt;) seems useful, but has the downside of reducing the readability of the Markdown text except for the simplest of diagrams.&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;mermaid&quot; src=&quot;https://mermaid.ink/svg/eyJjb2RlIjoiZ3JhcGggTFJcbkEoKEEpKSAtLT4gQigoQikpXG5BIC0tPiBDKChDKSkiLCJtZXJtYWlkIjpudWxsfQ&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Example of a Mermaid flowchart.&lt;/em&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-mermaid&quot;&gt;graph LR
  A((A)) --&amp;gt; B((B))
  A --&amp;gt; C((C))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;em&gt;The text version of the Mermaid flowchart is much less readable.&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;what-i-want-for-markdown&quot;&gt;What I want for Markdown&lt;/h2&gt;

&lt;p&gt;I love Markdown, and my world would be better if Markdown were supported &lt;strong&gt;everywhere&lt;/strong&gt;!&lt;/p&gt;

&lt;p&gt;By that I mean &lt;em&gt;all&lt;/em&gt; writing applications support Markdown syntax and extensions. Typing Markdown syntax would convert to the document’s format. Copy-and-paste commands would copy to Markdown, and intelligently convert pasted Markdown to the displayed formate.&lt;/p&gt;

&lt;p&gt;I want to be able to type &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;**really**&lt;/code&gt; in every application I use and see &lt;strong&gt;really&lt;/strong&gt;. That includes &lt;a href=&quot;msword&quot;&gt;Word&lt;/a&gt; — which is still the standard despite open-source and free document alternatives (&lt;em&gt;kudos!&lt;/em&gt;) and others like &lt;a href=&quot;outlook&quot;&gt;Outlook&lt;/a&gt;, &lt;a href=&quot;https://teams.microsoft.com/&quot;&gt;Teams&lt;/a&gt;, &lt;a href=&quot;[https://www.onenote.com](https://www.onenote.com/)&quot;&gt;OneNote&lt;/a&gt;, wikis, &lt;a href=&quot;[https://www.wix.com](https://www.wix.com/)&quot;&gt;Wix&lt;/a&gt;, &lt;a href=&quot;[https://medium.com](https://medium.com/)&quot;&gt;Medium&lt;/a&gt;, &lt;a href=&quot;https://www.atlassian.com/software/confluence&quot;&gt;Atlassian’s Confluence&lt;/a&gt;, &lt;a href=&quot;https://gsuite.google.com/products/gmail/&quot;&gt;Gmail&lt;/a&gt;, &lt;a href=&quot;https://en.wikipedia.org/wiki/Apple_Mail&quot;&gt;Apple mail&lt;/a&gt;, &lt;a href=&quot;https://evernote.com/&quot;&gt;Evernote&lt;/a&gt;, &lt;a href=&quot;https://docs.google.com/&quot;&gt;Google docs&lt;/a&gt; and others. (To be fair, it kind of works in some of these, and in others you can apply extensions that add some of the functionality.)&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;I really enjoy writing in Markdown because of the focus on writing content that is gives me. There are a couple of improvements that could be made, but given the opportunity of enhancing Markdown at the expense of its readability and simplicity, I would gladly keep it as is.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;I wrote the first draft of this article in Markdown. You can find a copy of the file &lt;a href=&quot;https://drive.google.com/file/d/16bg_tLdEJaTRdNclmfB23usqdyyHxfE0/view?usp=sharing&quot;&gt;here&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;more-information&quot;&gt;More information&lt;/h3&gt;

&lt;p&gt;&lt;a href=&quot;https://3-crowns-academy.teachable.com/p/build-your-first-document-classifier-with-machine-learning-in-python&quot;&gt;Click here to access my &lt;strong&gt;free course&lt;/strong&gt; on sentiment analysis.&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;© 2020 James Pearce, 3 Crowns Consulting.&lt;/p&gt;</content><author><name></name></author><category term="markdown" /><summary type="html">Why I love Markdown and want to see its use become more widespread</summary></entry><entry><title type="html">You should be using Azure Machine Learning designer</title><link href="http://www.3crownsconsulting.com.au/machine-learning/azure/2020/11/09/You-should-be-using-Azure-ML-designer.html" rel="alternate" type="text/html" title="You should be using Azure Machine Learning designer" /><published>2020-11-09T22:00:00+00:00</published><updated>2020-11-09T22:00:00+00:00</updated><id>http://www.3crownsconsulting.com.au/machine-learning/azure/2020/11/09/You-should-be-using-Azure-ML-designer</id><content type="html" xml:base="http://www.3crownsconsulting.com.au/machine-learning/azure/2020/11/09/You-should-be-using-Azure-ML-designer.html">&lt;h1 id=&quot;a-modern-update-with-a-familiar-look-and-feel-gives-data-scientists-the-best-of-both-worlds&quot;&gt;&lt;em&gt;A modern update with a familiar look and feel gives data scientists the best of both worlds&lt;/em&gt;&lt;/h1&gt;

&lt;p&gt;I have been a strong advocate of Microsoft’s &lt;a href=&quot;https://studio.azureml.net/&quot;&gt;Azure Machine Learning Studio (classic)&lt;/a&gt; since its release in 2015. It is a great tool, parts of it remain relevant and it is an excellent platform for students new to machine learning.&lt;/p&gt;

&lt;p&gt;It has an outstanding and its intuitive visual interface make it easy to use. It can be extended to be as powerful as you need it to be with R and Python scripts. Its ties to R are strong — this reflects Microsoft’s investment in R at the time. Microsoft published a slew of &lt;a href=&quot;https://gallery.azure.ai/&quot;&gt;galleries and examples&lt;/a&gt; that made Azure ML Studio an exceptional machine learning resource. It was — and still is — a go-to place to see how someone else has solved a problem in a way that is easy to understand.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/azure ML classic 01.png&quot; alt=&quot;A linear regression experiment to predict car prices in Azure Machine Learning Studio (classic)&quot; /&gt;&lt;br /&gt;
&lt;em&gt;A linear regression experiment to predict car prices in Azure Machine Learning Studio (classic)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Fast forward to late 2020 and the world has moved on. Microsoft has launched &lt;a href=&quot;https://azure.microsoft.com/en-us/services/machine-learning-service/&quot;&gt;an impressive range of machine learning services under the Azure banner&lt;/a&gt;, with strong ties to Python libraries. &lt;a href=&quot;https://azure.microsoft.com/en-us/pricing/details/machine-learning/&quot;&gt;&lt;strong&gt;They have now made them essentially free.&lt;/strong&gt;&lt;/a&gt; The focus of machine learning has tilted slightly &lt;a href=&quot;https://www.kdnuggets.com/2019/05/poll-top-data-science-machine-learning-platforms.html&quot;&gt;towards Python and away from R&lt;/a&gt;. Microsoft has started to deprecate its development in Azure Machine Learning Studio (classic) in favour of its new suite of machine learning services. The direction for R is through &lt;a href=&quot;https://docs.microsoft.com/en-us/sql/machine-learning/sql-server-machine-learning-services&quot;&gt;integration with SQL Server&lt;/a&gt; (named SQL Server Machine Learning Services).&lt;/p&gt;

&lt;p&gt;Is it me, or are the names confusing?&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Azure  Machine Learning Studio  (classic)&lt;/th&gt;
      &lt;th&gt;A  visual machine learning environment&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Azure  Machine Learning Services&lt;/td&gt;
      &lt;td&gt;Azure-based  API and integrated Python libraries for the full machine learning lifecycle&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;SQL Server  Machine Learning Services&lt;/td&gt;
      &lt;td&gt;R  and Python machine learning in SQL Server&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Come on Microsoft, bring back names we can relate to. Like &lt;a href=&quot;https://www.theverge.com/2019/3/22/18276923/microsoft-clippy-microsoft-teams-stickers-removal&quot;&gt;Clippy&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://cdn.windowsreport.com/wp-content/uploads/2014/10/clippy-windows-8-10.jpg&quot; alt=&quot;Fans of Apple were envious of the Windows-only Clippy&quot; /&gt;&lt;br /&gt;
&lt;em&gt;Fans of Apple were envious of the Windows-only Clippy&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;With all of this, faithful users were left with a choice: do I want ease of use or tight integration with Azure? You could have flexibility in Azure environments but were limited to code and the learning curve that comes with that. Or you could have easy development with a brilliant visual interface, but were limited in the Azure environments in terms of scale, compute, regions and storage for deploying models.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/how it was 2.png&quot; alt=&quot;The tradeoffs you had to make between Azure ML Studio and Azure ML Service.&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Enter the new visual interface for Azure Machine Learning known as (I think) &lt;a href=&quot;https://docs.microsoft.com/en-us/sql/machine-learning/sql-server-machine-learning-services&quot;&gt;Azure Machine Learning designer&lt;/a&gt; which is part of the new and equally-confusingly-named Azure Machine Learning Studio. If you are familiar with Azure Machine Learning Studio (classic), you will know how to use the new interface. It looks exactly the same &lt;em&gt;and&lt;/em&gt; it shares many features with its predecessor.&lt;/p&gt;

&lt;p&gt;Look at the screen grab below of the new Azure Machine Learning designer interface: the interface we love is back — with upgrades!&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/Azure ML designer.png&quot; alt=&quot;Azure Machine Learning designer&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Things are not quite the same. The ties with R have been replaced by stronger — and better — ties to Python. You have full control over your compute environment, and you have access to the full Azure run history of experiments. So you have flexibility with Azure, &lt;em&gt;plus&lt;/em&gt; a great visual coding interface which you probably already know.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/Azure run experiment.png&quot; alt=&quot;Details are available for your visual experiments.&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;To sum up, the new visual interface for Azure Machine Learning Service is a great addition to Microsoft’s Azure machine learning suite. You get the best of both worlds: power, flexibility and an intuitive interface &lt;strong&gt;plus&lt;/strong&gt; all the bells and whistles from the Azure Machine Learning suite. These include &lt;a href=&quot;https://docs.microsoft.com/en-us/azure/machine-learning/concept-automated-ml&quot;&gt;automated machine learning&lt;/a&gt;, &lt;a href=&quot;https://docs.microsoft.com/en-us/azure/machine-learning/how-to-machine-learning-interpretability&quot;&gt;interpretable machine learning&lt;/a&gt; and &lt;a href=&quot;&quot;&gt;model monitoring&lt;/a&gt;, all important capabilities for a modern machine learning suite.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/azure ml designer 2.png&quot; alt=&quot;azure ml designer 2&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Now Azure has an option that gives you power and flexibility with a visual interface&lt;/em&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;free-sentiment-analysis-course&quot;&gt;Free sentiment analysis course&lt;/h3&gt;

&lt;p&gt;&lt;a href=&quot;https://3-crowns-academy.teachable.com/p/build-your-first-document-classifier-with-machine-learning-in-python&quot;&gt;Click here to access my &lt;strong&gt;free course&lt;/strong&gt; on sentiment analysis.&lt;/a&gt;&lt;/p&gt;</content><author><name></name></author><category term="machine-learning" /><category term="azure" /><summary type="html">A modern update with a familiar look and feel gives data scientists the best of both worlds</summary></entry><entry><title type="html">Like sentiment analysis can detect sarcasm!</title><link href="http://www.3crownsconsulting.com.au/nlp/sentiment/2020/11/07/like-Sentiment-analysis-can-detect-sarcasm.html" rel="alternate" type="text/html" title="Like sentiment analysis can detect sarcasm!" /><published>2020-11-07T22:00:00+00:00</published><updated>2020-11-07T22:00:00+00:00</updated><id>http://www.3crownsconsulting.com.au/nlp/sentiment/2020/11/07/like-Sentiment-analysis-can-detect-sarcasm</id><content type="html" xml:base="http://www.3crownsconsulting.com.au/nlp/sentiment/2020/11/07/like-Sentiment-analysis-can-detect-sarcasm.html">&lt;p&gt;&lt;em&gt;Automatic sentiment analysis of texts has historically struggled to handle sarcasm well. Have advances in machine learning techniques made inroads to the detection of sarcasm?&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;You may have heard about &lt;a href=&quot;https://en.wikipedia.org/wiki/Sentiment_analysis&quot;&gt;sentiment analysis&lt;/a&gt;. It is a technique that analyses text and tries to establish whether the emotional state of the person who wrote it was &lt;strong&gt;positive&lt;/strong&gt; or &lt;strong&gt;negative&lt;/strong&gt;. It is a powerful way of understanding whether tweets regarding your product are trending positively or negatively. In turn, you can use this as an early warning system for problems as they arise. And there are myriad other uses.&lt;/p&gt;

&lt;p&gt;But English can be a strange language. One of the toughest jobs that automated sentiment detection has is when sarcasm is involved.&lt;/p&gt;

&lt;p&gt;This is not surprising. Actual people can have difficulty in detecting sarcasm. Think of Sheldon from &lt;em&gt;Big Bang Theory&lt;/em&gt; or Sean Murphy from &lt;em&gt;The Good Doctor&lt;/em&gt;. (And some have argued that Alanis Morrisette cannot recognise irony, &lt;a href=&quot;https://www.theatlantic.com/notes/2016/05/alanis-morissette-recognizes-its-not-ironic/481875/&quot;&gt;but maybe those people are too pedantic&lt;/a&gt;.)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://znculturecast.files.wordpress.com/2013/03/dc056b4f8c9493f3c64e6e0a85382b31.jpeg&quot; style=&quot;zoom:67%;&quot; /&gt;&lt;br /&gt;
&lt;em&gt;Dr Sheldon Cooper is really good at detecting sarcasm&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Consider the following quote.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;I really enjoyed this book and will recommend it to my friends and family.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;This quote contains &lt;em&gt;positive&lt;/em&gt; words (‘enjoyed’ and ‘recommended’) and reflects positive emotion.&lt;/p&gt;

&lt;p&gt;But consider this similar quote.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;I could not put this book down and will not hesitate to recommend it to family and friends.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;This sentence is a bit different.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;positive&lt;/th&gt;
      &lt;th&gt;negative&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;recommend&lt;/td&gt;
      &lt;td&gt;not (twice)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt;down&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt;hesitate&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Although overall it is a positive review, it contains some negative words. Simple sentiment calculations based on the occurrence of positive or negative words will misclassify this review.&lt;/p&gt;

&lt;p&gt;There are three negative words but only one positive word, giving a net negative result overall.&lt;/p&gt;

&lt;p&gt;Now consider a sarcastic review.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;I am glad the author chose to use overly long and flowery descriptions for each and every change of scene as I needed some scrap paper to line my pet parrot’s cage. Furthermore, the huge number of typographic errors was a refreshing change from reading professionally edited books. I recommend this to anyone who also enjoys long and unnecessary dental procedures.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;This is full of positive words — ‘glad’, ‘huge’, ‘refreshing’, ‘recommend’ and ‘enjoys’ — and contains &lt;em&gt;no&lt;/em&gt; negative words. Taken as a whole the review uses humourous sarcasm to give the book a negative review.&lt;/p&gt;

&lt;p&gt;This presents a huge challenge to standard sentiment calculation techniques.&lt;/p&gt;

&lt;p&gt;Over the past ten or so years, improvements in machine learning algorithms have brought us so much. Computer algorithms can now  &lt;a href=&quot;https://www.forbes.com/sites/bernardmarr/2018/08/24/will-machine-learning-ai-make-human-translators-an-endangered-species/#45b3d2df3902&quot;&gt;translate at a rate comparable to human translators&lt;/a&gt;, &lt;a href=&quot;https://deepfakenow.com/hollywood-deepfake-movies/&quot;&gt;do a better job than Hollywood&lt;/a&gt; of reanimating those actors who are no longer with us, &lt;a href=&quot;https://www.microsoft.com/en-us/research/blog/novel-object-captioning-surpasses-human-performance-on-benchmarks/&quot;&gt;caption images&lt;/a&gt; at a better success rate than people and generate &lt;a href=&quot;https://www.thisworddoesnotexist.com&quot;&gt;fascinating definitions&lt;/a&gt; for non-words.&lt;/p&gt;

&lt;p&gt;Understanding whether comments are sarcastic can be useful to know: some have suggested that sarcastic users may have more influence than other users. The question I want to explore further is:&lt;/p&gt;

&lt;h5 id=&quot;have-recent-advances-in-machine-learning-allowed-algorithms-to-correctly-and-reliably-detect-sarcasm&quot;&gt;Have recent advances in machine learning allowed algorithms to correctly and reliably detect sarcasm?&lt;/h5&gt;

&lt;p&gt;Before getting too far into this topic, what &lt;em&gt;is&lt;/em&gt; sarcasm? You can look it up on &lt;a href=&quot;https://duckduckgo.com&quot;&gt;Duck Duck Go&lt;/a&gt; if you like, but here is my take on it.&lt;/p&gt;

&lt;p&gt;Sarcasm is a form of irony used to convey the opposite of what is stated in some sense. It can be used as &lt;a href=&quot;https://www.independent.co.uk/news/science/sarcasm-how-lowest-form-wit-actually-makes-people-brighter-and-more-creative-10416281.html&quot;&gt;the lowest form of wit&lt;/a&gt; or to be humorous; it is often used as an insult or to mock but also to display irritation. In conversation, sarcasm is usually accompanied by gestures, tonal stress or eye-rolls.&lt;/p&gt;

&lt;p&gt;When analysing text in situations where we do not get the benefits of tonal or visual cues, we need to rely &lt;em&gt;only&lt;/em&gt; on what was written. Sometimes a writer will include additional clues like emoticons and emoji to signal their true emotion, which can help.&lt;/p&gt;

&lt;p&gt;One of the problems with detecting sarcasm is having some baseline of truth against which to compare. We need some data on what comments are sarcastic and which are not. As sarcasm is not universally understood — and we know Sheldon would not be an effective arbiter of sarcasm — any pre-existing data set with sarcastic–non-sarcastic labels is bound to contain errors.&lt;/p&gt;

&lt;p&gt;A relatively recent data set by Oprea and Magdy &lt;a href=&quot;https://arxiv.org/pdf/2009.13080.pdf&quot;&gt;(&lt;em&gt;Reactive Supervision&lt;/em&gt;)&lt;/a&gt; contains self-described sarcastic comments along with an explanation of why the author thought the comment was sarcastic. I will be a useful resource for training and evaluating sarcasm detectors, but even this will present some problems:  the data set does not include the context of the original comment, and &lt;a href=&quot;http://documents.mx/documents/yeah-right-a-linguistic-analysis-of-self-reported-sarcastic-messages-and-their-contexts.html&quot;&gt;people themselves are unreliable at reporting sarcasm in their own comments&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;There is a lesson here for aspiring writers. Avoid sarcasm if you want to be understood well: your audience may not comprehend the signal you are hiding in the noise of sarcasm.&lt;/p&gt;

&lt;p&gt;Current strategies to detect sarcasm in comments generally look to explore knowledge outside of the comment itself (I use the word ‘comments’ here to include reviews, tweets and other short texts). For example,&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;www.crowdanalyzer.com&quot;&gt;separate the analysis of emotion from the sentiment analysis&lt;/a&gt;;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://deepai.org/publication/the-role-of-conversation-context-for-sarcasm-detection-in-online-interactions&quot;&gt;understand the context in which the comment was made&lt;/a&gt;;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://towardsdatascience.com/sarcasm-detection-with-nlp-cbff1723f69a&quot;&gt;examine whether an author has been sarcastic in the past&lt;/a&gt;;&lt;/li&gt;
  &lt;li&gt;determine whether the comment seems to contradict itself and contains many positive and negative words;&lt;/li&gt;
  &lt;li&gt;determine whether the comment seems to contradict generally known facts (sometimes split into &lt;a href=&quot;https://www.toptal.com/deep-learning/4-sentiment-analysis-accuracy-traps&quot;&gt;numeric&lt;/a&gt; and non-numeric comments); and&lt;/li&gt;
  &lt;li&gt;compare the structure of the comment with known structures of sarcasm (for example, &lt;a href=&quot;[here](https://www.crowdanalyzer.com/blog/sentiment-analysis)&quot;&gt;going from positive to negative&lt;/a&gt;, &lt;a href=&quot;https://towardsdatascience.com/sarcasm-detection-with-nlp-cbff1723f69a&quot;&gt;a polite but contrasting tone&lt;/a&gt; and &lt;a href=&quot;https://www.themarysue.com/sarcasm-detecting-algorithm-online/&quot;&gt;excessive use of capitals and exclamation marks&lt;/a&gt;).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;And of course, you could try multiple approaches. (See &lt;a href=&quot;https://www.researchgate.net/publication/325843750_A_COMPREHENSIVE_STUDY_ON_SARCASM_DETECTION_TECHNIQUES_IN_SENTIMENT_ANALYSIS/link/5bb0db42a6fdccd3cb7f80e0/download&quot;&gt;here&lt;/a&gt; for some more detail on sarcasm structure.)&lt;/p&gt;

&lt;p&gt;A complementary approach is to use &lt;a href=&quot;https://github.com/SenticNet/CASCADE--ContextuAl-SarCAsm-DEtector&quot;&gt;deep learning architectures&lt;/a&gt; to train sarcasm detectors. These are showing some promise, especially if combined with some of the approaches above. But the results in any of the papers are not blowing me away in the same way that other deep learning models do.&lt;/p&gt;

&lt;p&gt;It would seem from the literature, code, blog posts and other informative sources that we have made some inroads into sarcasm detection, but there is still a way to go.&lt;/p&gt;

&lt;p&gt;For some fun, I thought I would try an extremely non-scientific approach to evaluating two online sarcasm detectors. In both of these online detectors, you can visit the website and enter your text. (Obviously these detectors cannot account for context.) To test them out, I used two sarcastic book reviews from Amazon as well as the example from above.&lt;/p&gt;

&lt;p&gt;Here are the results:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Comment&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;[The sarcasm detector](http://www.thesarcasmdetector.com) (the score is from -100 to 100; the higher the more sarcastic)&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;[Parallel Dots API](https://www.paralleldots.com/sarcasm-detection)  (0% to 100%; the higher the more sarcastic)&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;I am glad the author chose to use overly long and flowery descriptions for each and every change of scene as I needed some scrap paper to line my pet parrot’s cage. Furthermore, the huge number of typographic errors was a refreshing change from reading professionally edited books. I recommend this to anyone who also enjoys long and unnecessary dental procedures.&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;-46&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;44%&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;This book works if you need something to read on a plane trip from Florida to Ohio.  It held my interest to an extent.  However, I got the feeling he knocked this one off in about the time it takes to type it.  It takes about 3 minutes to figure out his daughter is coming home for Christmas.  Wait for the paperback.&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;-71&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;58%&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;One need only to have listened to the oblique babbling of most corporate managers to realize that this is their Bible. Admittedly it will help your career.  You will learn how to speak out of both sides of your mouth, appear agreeable at all times, and engage in all manner of corporate BS.  Everyone will like you, except for those ne&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt; &lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt; &lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;The commercial version from Parallel Dots worked a little better than &lt;em&gt;The Sarcasm Detector&lt;/em&gt;, but both are obviously struggling with the meaning of these reviews.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;Sarcasm detection has improved but can get better.&lt;/p&gt;

&lt;p&gt;There is potential in combining various approaches, using deep learning architectures, accounting for the conversation and user context and codifying patterns of known sarcasms to improve the current state of detection.&lt;/p&gt;

&lt;p&gt;But keep in mind the point is moot. Sarcasm is only an effective means of communication when the author and reader share context. Without this, humans — not just algorithms — will fail to get the intended message.&lt;/p&gt;

&lt;p&gt;If you are analysing sentiment, make sure you use the emotion of any included emoji or emoticons to help your analysis. Include a rating field if you want better understanding of your customers’ true sentiment.&lt;/p&gt;

&lt;p&gt;If you are a writer, &lt;em&gt;be clear&lt;/em&gt;.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;**NOTE: For some reason this post is rendering strangely. I am aware of it and apologise. I hope I can fix it soon.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;more-information&quot;&gt;More information&lt;/h3&gt;

&lt;p&gt;&lt;a href=&quot;https://3-crowns-academy.teachable.com/p/build-your-first-document-classifier-with-machine-learning-in-python&quot;&gt;Click here for more information on sentiment analysis and to access my &lt;strong&gt;free course&lt;/strong&gt;.&lt;/a&gt;
er-do goods-who abhor pretension and deceit.  And, most importantly, you will get that raise!  After all, get real, being honest, principled and lucid won&lt;/p&gt;

&lt;p&gt;The commercial version from Parallel Dots worked a little better than &lt;em&gt;The Sarcasm Detector&lt;/em&gt;, but both are obviously struggling with the meaning of these reviews.&lt;/p&gt;

&lt;h2 id=&quot;conclusion-1&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;Sarcasm detection has improved but can get better.&lt;/p&gt;

&lt;p&gt;There is potential in combining various approaches, using deep learning architectures, accounting for the conversation and user context and codifying patterns of known sarcasms to improve the current state of detection.&lt;/p&gt;

&lt;p&gt;But keep in mind the point is moot. Sarcasm is only an effective means of communication when the author and reader share context. Without this, humans — not just algorithms — will fail to get the intended message.&lt;/p&gt;

&lt;p&gt;If you are analysing sentiment, make sure you use the emotion of any included emoji or emoticons to help your analysis. Include a rating field if you want better understanding of your customers’ true sentiment.&lt;/p&gt;

&lt;p&gt;If you are a writer, &lt;em&gt;be clear&lt;/em&gt;.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;**NOTE: For some reason this post is rendering strangely. I am aware of it and apologise. I hope I can fix it soon.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;more-information-1&quot;&gt;More information&lt;/h3&gt;

&lt;p&gt;&lt;a href=&quot;https://3-crowns-academy.teachable.com/p/build-your-first-document-classifier-with-machine-learning-in-python&quot;&gt;Click here for more information on sentiment analysis and to access my &lt;strong&gt;free course&lt;/strong&gt;.&lt;/a&gt;
t pay the rent and may even get you a pink slip. If you want to &quot;get ahead&quot; buy this book! If you are like me and amuse yourself by reading the kind of obfuscate and dissimulating language found in those emails from managers that arrive in your workplace computer, get this book for a good laugh! Dale Carnegie is the St. Paul of American Yuppies. |                                                          -36 |                                                          27% |&lt;/p&gt;

&lt;p&gt;The commercial version from Parallel Dots worked a little better than &lt;em&gt;The Sarcasm Detector&lt;/em&gt;, but both are obviously struggling with the meaning of these reviews.&lt;/p&gt;

&lt;h2 id=&quot;conclusion-2&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;Sarcasm detection has improved but can get better.&lt;/p&gt;

&lt;p&gt;There is potential in combining various approaches, using deep learning architectures, accounting for the conversation and user context and codifying patterns of known sarcasms to improve the current state of detection.&lt;/p&gt;

&lt;p&gt;But keep in mind the point is moot. Sarcasm is only an effective means of communication when the author and reader share context. Without this, humans — not just algorithms — will fail to get the intended message.&lt;/p&gt;

&lt;p&gt;If you are analysing sentiment, make sure you use the emotion of any included emoji or emoticons to help your analysis. Include a rating field if you want better understanding of your customers’ true sentiment.&lt;/p&gt;

&lt;p&gt;If you are a writer, &lt;em&gt;be clear&lt;/em&gt;.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;**NOTE: For some reason this post is rendering strangely. I am aware of it and apologise. I hope I can fix it soon.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;more-information-2&quot;&gt;More information&lt;/h3&gt;

&lt;p&gt;&lt;a href=&quot;https://3-crowns-academy.teachable.com/p/build-your-first-document-classifier-with-machine-learning-in-python&quot;&gt;Click here for more information on sentiment analysis and to access my &lt;strong&gt;free course&lt;/strong&gt;.&lt;/a&gt;&lt;/p&gt;</content><author><name></name></author><category term="NLP" /><category term="sentiment" /><summary type="html">Automatic sentiment analysis of texts has historically struggled to handle sarcasm well. Have advances in machine learning techniques made inroads to the detection of sarcasm?</summary></entry></feed>